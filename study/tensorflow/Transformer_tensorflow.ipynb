{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f33d43b6-b663-4fc8-9cc2-5829f9e75198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5eae50-8474-4f2c-a0db-e0c65efe5fab",
   "metadata": {},
   "source": [
    "## 구조 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7151cd34-d196-461e-b3b8-338d2a29b39a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTransformer(num_layers, input_src, input_tgt, max_vocab, seq_length, dim_embedding, num_head, dff)\\n    Encoder(num_layers, input_src, max_vacab, seq_length, dim_embedding, pad_mask, num_head, dff)\\n        Encoder layer(input_src, max_vacab, seq_length, dim_embedding, pad_mask, num_head, dff)\\n            vectorization(input_src, max_vacab, seq_length)\\n            embedding(input_src2, max_vacab, dim_embedding)\\n            positional encoding(input_src3, position=seq_length, depth=dim_embedding)\\n            multi-head self attention(input_src4, dim_model=dim_embedding, num_head)\\n                pad_mask\\n            add&normalization(input_src5)\\n            feed forward neural network(input_src6, dim_model, dff)\\n            add&normalization(input_src7)\\n    Decoder(num_layers, num_layers, input, max_vacab, seq_length, dim_embedding, pad_mask, num_head, dff)\\n        Decoder layer(input_tgt, max_vacab, seq_length, dim_embedding, pad_mask, num_head, dff, enc_out)\\n            vectorization(input_tgt, max_vacab, seq_length)\\n            embedding(input_tgt2, max_vacab, dim_embedding)\\n            positional encoding(input_tgt3, position=seq_length, depth=dim_embedding)\\n            masked multi-head self attention(input_tgt4, dim_model=dim_embedding, num_head)\\n                mask, pad_mask\\n            add&normalization(input_tgt5)\\n            cross multi-head self attention(input_tgt6, enc_output, pad_mask, dim_model=dim_embedding, num_head)\\n            add&normalization(input_tgt7)\\n            feed forward neural network(input_tgt8, dim_model, dff)\\n            add&normalization(input_tgt9)\\n    Generator\\n        Linear(dec_out)\\n        Softmax(linear_out)\\n            \\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Transformer(num_layers, input_src, input_tgt, max_vocab, seq_length, dim_embedding, num_head, dff)\n",
    "    Encoder(num_layers, input_src, max_vacab, seq_length, dim_embedding, pad_mask, num_head, dff)\n",
    "        Encoder layer(input_src, max_vacab, seq_length, dim_embedding, pad_mask, num_head, dff)\n",
    "            vectorization(input_src, max_vacab, seq_length)\n",
    "            embedding(input_src2, max_vacab, dim_embedding)\n",
    "            positional encoding(input_src3, position=seq_length, depth=dim_embedding)\n",
    "            multi-head self attention(input_src4, dim_model=dim_embedding, num_head)\n",
    "                pad_mask\n",
    "            add&normalization(input_src5)\n",
    "            feed forward neural network(input_src6, dim_model, dff)\n",
    "            add&normalization(input_src7)\n",
    "    Decoder(num_layers, num_layers, input, max_vacab, seq_length, dim_embedding, pad_mask, num_head, dff)\n",
    "        Decoder layer(input_tgt, max_vacab, seq_length, dim_embedding, pad_mask, num_head, dff, enc_out)\n",
    "            vectorization(input_tgt, max_vacab, seq_length)\n",
    "            embedding(input_tgt2, max_vacab, dim_embedding)\n",
    "            positional encoding(input_tgt3, position=seq_length, depth=dim_embedding)\n",
    "            masked multi-head self attention(input_tgt4, dim_model=dim_embedding, num_head)\n",
    "                mask, pad_mask\n",
    "            add&normalization(input_tgt5)\n",
    "            cross multi-head self attention(input_tgt6, enc_output, pad_mask, dim_model=dim_embedding, num_head)\n",
    "            add&normalization(input_tgt7)\n",
    "            feed forward neural network(input_tgt8, dim_model, dff)\n",
    "            add&normalization(input_tgt9)\n",
    "    Generator\n",
    "        Linear(dec_out)\n",
    "        Softmax(linear_out)\n",
    "            \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5386bc1e-41ec-43a8-9a54-91ac3d0b6d67",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd749dfe-9726-4e5b-b17d-7a219b3f6ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(inputs):\n",
    "    #input의 형태는 문장을 vectorization -> embedding한 (batch_size, seq_length, dim_embedding)의 형태로 입력이 됨\n",
    "    #batch_size는 모델의 파라미터로 한번에 처리할 문장의 갯수\n",
    "    #seq_length는 한 문장에서 처리할 Token의 숫자\n",
    "    #dim_embedding은 한개의 token을 몇 차원으로 embedding하는지 나타내는 값\n",
    "    seq_length = inputs.shape[-2]\n",
    "    dim_embedding = inputs.shape[-1]\n",
    "    positional_encoding = np.zeros((seq_length, dim_embedding))\n",
    "\n",
    "    \n",
    "    #(seq_length, dim_embedding)의 구조로 된 문장을 positional encoding을 하기 위해 pos(행), i(열) 좌표(?) 위치를 계산\n",
    "    pos = np.arange(seq_length)[:,np.newaxis] #행의 번호\n",
    "    i = np.arange(dim_embedding)[np.newaxis,:] #열의 번호\n",
    "    d_model = dim_embedding\n",
    "\n",
    "    #positional encoding angle 공식\n",
    "    angle = pos*1/np.power(10000,(2 * (i // 2) / np.float32(dim_embedding)))\n",
    "\n",
    "    #i가 짝수 일때는 sin, i가 홀수 일때는 cos으로 계산하여, 위치별로 더해줄 positional encoding 값을 계산\n",
    "    positional_encoding[:, 0::2] = np.sin(angle[:, 0::2])\n",
    "    positional_encoding[:, 1::2] = np.cos(angle[:, 1::2])\n",
    "    positional_encoding[tf.newaxis, ...] #(batch_size, seq_length, dim_embedding)형태로 만들어 주기 위해 batch_size 부분에 해당하는 np.newaxis 추가\n",
    "\n",
    "    return tf.cast(positional_encoding, dtype=tf.float32) #output 차원 = (batch_size, seq_length, dim_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5687ee0-8020-4436-bda9-8562a07da3a3",
   "metadata": {},
   "source": [
    "## scaled_dot_product & split_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d012ecb6-c5be-420a-8305-c1cd8e516365",
   "metadata": {},
   "outputs": [],
   "source": [
    "#self attention에서는 input으로 embedded positional encoded sentence를 받고 query, key, value를 만들어 self attention score를 얻어낸다.\n",
    "#필요한 input: input\n",
    "\n",
    "#input을 받아서 query, key, value를 만든다.\n",
    "#matmul : matmul(query,key.transpose)\n",
    "#scaling : matmul/tf.sqrt(dim_k)\n",
    "#pad_mask : scaled + pad_mask\n",
    "#softmax : softmax(pad_masked)\n",
    "#output : matmul(softmax, value)\n",
    "\n",
    "def create_pad_mask(x):\n",
    "    #여기서 input 값인 x는 vectorize된 문장이 들어가야 함\n",
    "    #input 형태 : (batch_size, seq_length) -> [[1,2,3,1,0,0,0],[3,2,4,0,0,0,0]] 이런 형태로 입력\n",
    "    #matirx에서 x값이 0과 같은 경우, True, 아니면 False를 반환하고, tf.cast를 통해서 boolen을 float32로 변환(True = 1.0, False=0.0)\n",
    "    mask = tf.cast(tf.math.equal(x, 0), dtype=tf.float32)\n",
    "    #return은 multi-head self attention에서 사용하기 위해 (batch_size, num_head, seq_length, seq_length)의 형태로 만들어줌\n",
    "    #batch_size는 문장의 숫자가 되고, num_head는 multi-head를 사용할 때 필요한 차원이고, scaled_dot_product에서 mask를 적용해야 할 matmul_qk의 차원이 seq_length, seq_length인데 우리는 Column에다가 mask를 적용할 것이기 떄문에, (Batch_size, 1, 1, seq_length)의 차원으로 만들어줌\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :] #output 차원 (batch_size, 1, 1, seq_length)\n",
    "\n",
    "\n",
    "def scaled_dot_product(query, key, value, pad_mask=None):\n",
    "    # query와 key의 transpose dot product이후 scaling\n",
    "    # 입력 query, key, value의 차원 = (batch_size, seq_length, dim_k)\n",
    "    # 즉 입력은 Embedding layer와 Positional encoding layer를 통과한 후의 데이터가 입력 되면 됨\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b = True) #차원 = (batch_szie, seq_length, seq_length)\n",
    "    dim_k = tf.cast(key.shape[-1], tf.float32)\n",
    "    scaled_matmul_qk = matmul_qk / tf.math.sqrt(tf.cast(dim_k, tf.float32)) #차원 = (batch_size, seq_length, seq_length)\n",
    "\n",
    "    if pad_mask is not None:\n",
    "        # 패딩된 위치에 -1e9를 더해 softmax에서 0에 가까운 값이 되도록 함 \n",
    "        # pad mask의 차원은 (batch_size, num_head, seq_length, seq_length)이므로, multi_head self attention에서 사용해야함\n",
    "        scaled_matmul_qk += (pad_mask * -1e9) #차원은 (batch_size, (num_head), seq_length, seq_length)\n",
    "    \n",
    "    # 소프트맥스 적용\n",
    "    softmax_qk = tf.nn.softmax(scaled_matmul_qk, axis=-1) #행 방향으로 softmax를 하고, 차원은 변경 없이 (batch_size, (num_head), seq_length, seq_length)\n",
    "    \n",
    "    # Attention 결과 계산\n",
    "    attention = tf.matmul(softmax_qk, value) #차원은 (batch_size, (num_head), seq_length, dim_k)\n",
    "    \n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15478aa3-be3f-4728-850b-59dab8ed0ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(x, batch_size, num_head, depth):\n",
    "    #multi-head self attention을 하기 원래 query, key, value(batch_size, seq_length, dim_k)를 num_haed만큼 concat을 시켜서 최종 attention을 얻어야 한다.\n",
    "    #이를 병렬 처리 하기 위해서 query, key, value를 (batch_size, seq_length, num_head, depth(뒤에서는 dim_k=dim_model/num_head로 표현))의 형태로 한번에 변환을 해줌\n",
    "    #reshape과정에서 데이터가 바뀌면 안되기 때문에, 원래의 query (batch_size, seq_length, dim_model)에서 dim_model을 num_head로 잘라내고 남은 depth를 마지막 차원에 둠, -1을 넣는 것은 seq_length 자리임, 자동 계산되고 남은 숫자가 seq_length에 들어가게 됨\n",
    "    #마지막으로 병렬처리르 할 수 있게 (batch, seq_length, num_head, depth)인 데이터를 (batch_size, num_head, seq_length, depth)로 변경해서, num_head만큼 동시에 병렬처리를 진행\n",
    "    x = tf.reshape(x, (batch_size, -1, num_head, depth))\n",
    "    return tf.transpose(x, perm=[0,2,1,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4759342-06aa-4e23-b70d-a22835bde8b3",
   "metadata": {},
   "source": [
    "## multi-head self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8617a61c-dfe8-47b3-9922-67ed3a7cf4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiheadselfattention():\n",
    "    def __init__(self, dim_model, num_head):\n",
    "        self.dim_model = dim_model\n",
    "        self.num_head = num_head\n",
    "        self.depth = dim_model // num_head\n",
    "\n",
    "        # query, key, value에 Dense layer를 생성\n",
    "        self.q_layer = tf.keras.layers.Dense(dim_model)\n",
    "        self.k_layer = tf.keras.layers.Dense(dim_model)\n",
    "        self.v_layer = tf.keras.layers.Dense(dim_model)\n",
    "\n",
    "        # 최종 계산된 attention output을 입력값 차원과 동일하게 dim_embedding으로 바꿔주기 위해서 Dense_layer 사용\n",
    "        self.dim_embedding = dim_model\n",
    "        self.dense_layer = tf.keras.layers.Dense(self.dim_embedding)\n",
    "            \n",
    "    def __call__(self, query, key, value, pad_mask):\n",
    "        #batch_size를 입력된 data를 통해서 확인\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        #위에서 만든 각각의 Dense layer를 이용하여 query, key, value 계산\n",
    "        q = self.q_layer(query)\n",
    "        k = self.k_layer(key)\n",
    "        v = self.v_layer(value)\n",
    "\n",
    "        #한번에 계산이된 dim_model 차원의 query, key, value 값을 위에서 정의한 split_head 함수로 num_head개의 depth 차원으로 분할\n",
    "        q = split_heads(q, batch_size, self.num_head, self.depth)\n",
    "        k = split_heads(k, batch_size, self.num_head, self.depth)\n",
    "        v = split_heads(v, batch_size, self.num_head, self.depth)\n",
    "\n",
    "        #위에서 정의한 scaled_dot_product에 q, k, v, 그리고 input data에 맞게 생성된 pad_mask를 입력하여 self attention 연산\n",
    "        attention = scaled_dot_product(q, k, v, pad_mask) #입력된 값은 (batch_size, num_head, seq_length, depth)차원이고, output역시 동일하게 (batch_size, num_head, seq_length, depth)가 된다.\n",
    "        attention_scaled = tf.transpose(attention, perm=[0,2,1,3]) #위의 output이 (batch_size, num_head, seq_length, depth)이기 떄문에 다시 concat을 하고 원래 최초에 들어온 input인 (batch_size, seq_length, dim_model) 차원을 맞추기 위해서 (batch_size, seq_length, num_head, depth)로 차원을 바꿔준다.\n",
    "        attention_concat = tf.reshape(attention_scaled, (batch_size, -1, self.dim_model)) #위에서 바꾼 차원을 값을 num_head와 depth를 합쳐 (batch_size, seq_length, dim_model로 바꿔줌\n",
    "        attention_output = self.dense_layer(attention_concat)\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c1e2023-769b-4660-8cfd-3dab438db776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_sample(batch_size=32, seq_length=40, embedding_dim=512):\n",
    "    \"\"\"\n",
    "    임베딩된 텍스트를 시뮬레이션하는 샘플 데이터를 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "    - batch_size: 배치 크기\n",
    "    - seq_length: 시퀀스 길이\n",
    "    - embedding_dim: 임베딩 차원\n",
    "    \n",
    "    Returns:\n",
    "    - 생성된 샘플 데이터 (shape: [batch_size, seq_length, embedding_dim])\n",
    "    \"\"\"\n",
    "    # 정규 분포를 사용하여 랜덤한 임베딩 생성\n",
    "    sample_data = tf.random.normal(shape=[batch_size, seq_length, embedding_dim])\n",
    "    \n",
    "    return sample_data\n",
    "\n",
    "# 샘플 데이터 생성\n",
    "sample_embeddings = create_embedding_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eba42ca-1463-4708-b4b9-cbcf169800f8",
   "metadata": {},
   "source": [
    "## Fead Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49ae6f70-68da-4225-bfcf-c8934794fe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feadforwardneuralnetwork():\n",
    "    def __init__(self, dim_model, dim_ff):\n",
    "        self.dim_ff = dim_ff\n",
    "        self.dim_model = dim_model\n",
    "        self.dense_layer1 = tf.keras.layers.Dense(self.dim_ff, activation='relu')\n",
    "        self.dense_layer2 = tf.keras.layers.Dense(self.dim_model)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        dense_layer2 = tf.keras.layers.Dense(self.dim_model)\n",
    "        x1 = self.dense_layer1(x)\n",
    "        output_ffnn = self.dense_layer2(x1)\n",
    "        return output_ffnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0271eca8-17ac-4f9f-9dbc-c133fbc83b30",
   "metadata": {},
   "source": [
    "## Encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2afd4a4e-0c2c-456d-b30c-11edf2b80ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoderlayer():\n",
    "    def __init__(self, dim_model, num_head, dim_ff):\n",
    "        self.dim_model = dim_model\n",
    "        self.num_head = num_head\n",
    "        self.dim_ff = dim_ff\n",
    "        \n",
    "        self.mha = Multiheadselfattention(self.dim_model, self.num_head)\n",
    "        self.ffnn = Feadforwardneuralnetwork(self.dim_model, self.dim_ff)\n",
    "\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def __call__(self, x, pad_mask):\n",
    "        attention = self.mha(x, x, x, pad_mask)\n",
    "\n",
    "        attention_norm = self.norm1(attention+x)\n",
    "\n",
    "        output_ffnn = self.ffnn(attention_norm)\n",
    "\n",
    "        output_ffnn_norm = self.norm2(output_ffnn+attention_norm)\n",
    "\n",
    "        return output_ffnn_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05f661b-78b9-4508-9e8a-116f8d048322",
   "metadata": {},
   "source": [
    "## look ahead mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef977c7e-2c20-42f3-b5dd-ca3f1185f20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookaheadmask(seq_length):\n",
    "    #tf.ones((seq_length, seq_length))는 seq_length*seq_length의 matrix에 1을 가득 채운 것임\n",
    "    #tf.linalg.band_part(matrix, num_lower, num_upper)에서 num_lower는 대각선 아래 유지할 행의 개수, num_upper는 대각선 위 유지할 열의 개수이고, -1을 입력하면 전체를 의미하고, 유지하지 않는 부분은 0으로 채움\n",
    "    #1-tf.linalg~~~ 를 진행하는 이유는 나중에 1e-9를 곱해주어서 softmax에서 0을 만들기 위함\n",
    "    mask = 1-tf.linalg.band_part(tf.ones((seq_length, seq_length)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "def create_mask(x):\n",
    "    #padding과 look ahead mask를 한번에 만들어주는 함수\n",
    "    #input으로 사용되는 x는 vectorized된 문장이다. \n",
    "    #input 형태 : (batch_size, seq_length) -> [[1,2,3,1,0,0,0],[3,2,4,0,0,0,0]] 이런 형태로 입력\n",
    "    temp_mask = create_lookaheadmask(tf.shape(x)[1]) #temp_mask는 위의 create_lookaheadmask를 그대로 실행해서 만듬\n",
    "    reverse_tar = tf.cast(tf.math.equal(x, 0),dtype=tf.float32) #입력이 되는 tar 문장의 pad mask이다. 즉 pad 부분을 1로, 단어가 있는 부분을 0으로 만드는 식\n",
    "    reverse_tar = reverse_tar[:,tf.newaxis,tf.newaxis,:] #나중에 (batch_size, num_head, seq_length, seq_length) 형태로 더해주기 위해 형태 변형 -> (batch_size, 1, 1, seq_length)형태\n",
    "    look_ahead_mask = tf.maximum(reverse_tar, temp_mask) #최종형태는 pad와 look_ahead 부분을 모두 가리는 mask\n",
    "    return look_ahead_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57511d61-bf12-4d3f-91bd-46247dfe8a5d",
   "metadata": {},
   "source": [
    "## Decoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7340831-e728-463c-8858-557434bd8eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_layer():\n",
    "    def __init__(self, dim_model, num_head, dim_ff):\n",
    "        self.dim_model = dim_model\n",
    "        self.dim_ff = dim_ff\n",
    "        self.num_head = num_head\n",
    "\n",
    "        self.mha1 = Multiheadselfattention(self.dim_model, self.num_head)\n",
    "        self.mha2 = Multiheadselfattention(self.dim_model, self.num_head)\n",
    "        self.ffnn = Feadforwardneuralnetwork(self.dim_model, self.dim_ff)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def __call__(self, tar, out_enc, pad_mask, look_ahead_mask):        \n",
    "        attention = self.mha1(tar, tar, tar, look_ahead_mask)\n",
    "\n",
    "        attention_norm = self.norm1(attention+tar)\n",
    "\n",
    "        cross_attention = self.mha2(attention_norm, out_enc, out_enc, pad_mask)\n",
    "\n",
    "        cross_attention_norm = self.norm2(cross_attention + attention_norm)\n",
    "\n",
    "        ffnn = self.ffnn(cross_attention_norm)\n",
    "\n",
    "        ffnn_norm = self.norm3(ffnn+cross_attention_norm)\n",
    "\n",
    "        return ffnn_norm       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0e072d-be4e-4d0c-9eee-0913cf70336e",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "659844a1-2020-420c-902d-465a9125994f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder():\n",
    "    def __init__(self, dim_model, num_head, dim_ff, num_layer):\n",
    "        self.dim_model = dim_model\n",
    "        self.num_head = num_head\n",
    "        self.dim_ff = dim_ff\n",
    "        self.num_layer = num_layer\n",
    "\n",
    "        self.encoder_block = [Encoderlayer(self.dim_model,self.num_head,self.dim_ff) for _ in range(self.num_layer)]\n",
    "\n",
    "    def __call__(self, x, pad_mask):\n",
    "        for i in range(self.num_layer):\n",
    "            x = self.encoder_block[i](x, pad_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dc3c19-186f-4cea-b6c9-c8fb5548682a",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46a4b044-f66b-42bf-9ea1-258cac99fff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder():\n",
    "    def __init__(self, dim_model, num_head, dim_ff, num_layer):\n",
    "        self.dim_model = dim_model\n",
    "        self.num_head = num_head\n",
    "        self.dim_ff = dim_ff\n",
    "        self.num_layer = num_layer\n",
    "\n",
    "        self.decoder_block = [Decoder_layer(self.dim_model,self.num_head,self.dim_ff) for _ in range(self.num_layer)]\n",
    "\n",
    "    def __call__(self, tar, out_enc, pad_mask, look_ahead_mask):\n",
    "        for i in range(self.num_layer):\n",
    "            out = self.decoder_block[i](tar,out_enc, pad_mask, look_ahead_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6878a896-6549-4b7e-923a-cafe2e2d5d39",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16acf24d-8577-45e8-9643-a17a957b63a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    #Transforemr에서 사용하는 Parameter는 여러군데에서 사용이 되어서 혼동이 될 수 있는데 아래를 참고하자.\n",
    "    #dim_model : dim_model 자체는 Multi-head attention에서 사용을 하는 dimension인데, input = output을 유지하기 위해서, dim_embedding, depth*num_head와 동일한 값을 가진다.\n",
    "    #num_head : multi-head에서 몇 개의 head로 병렬 연산을 할 것인지 결정하는 것이다.\n",
    "    #dim_ff : ffnn의 Dense layer에서 사용되는 dimension으로 특정되는 숫자가 있지는 않다.\n",
    "    #num_layer : encoder와 decoder layer들의 반복 횟수이다.\n",
    "    #max_token : vectorize할 때 최대로 사용할 token의 숫자이고, embedding할 때도 동일하게 가져간다.\n",
    "    #seq_len : 한 문장 안에서 input으로 처리할 최대의 단어 숫자를 의미\n",
    "    def __init__(self, dim_model, num_head, dim_ff, num_layer, input_vocab_size, output_vocab_size, seq_len):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.dim_model = dim_model\n",
    "        self.num_head = num_head\n",
    "        self.dim_ff = dim_ff\n",
    "        self.num_layer = num_layer\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.encoder = Encoder(self.dim_model, self.num_head, self.dim_ff, self.num_layer)\n",
    "        self.decoder = Decoder(self.dim_model, self.num_head, self.dim_ff, self.num_layer)\n",
    "        # self.vectorizer_src = tf.keras.layers.TextVectorization(max_tokens=self.max_token,output_mode='int',output_sequence_length=self.seq_len)\n",
    "        # self.vectorizer_tar = tf.keras.layers.TextVectorization(max_tokens=self.max_token,output_mode='int',output_sequence_length=self.seq_len)\n",
    "        self.embedding_src = tf.keras.layers.Embedding(input_dim=self.input_vocab_size, output_dim=self.dim_model)\n",
    "        self.embedding_tar = tf.keras.layers.Embedding(input_dim=self.output_vocab_size, output_dim=self.dim_model)\n",
    "        self.final_layer = tf.keras.layers.Dense(self.output_vocab_size)\n",
    "\n",
    "    def __call__(self, src_lang, tar_lang):\n",
    "        #src_lang, tar_lang은 vectorized된 input을 받는게 좋다. 매번 실행할 때마다, vocab을 학습하고, vectorize를 하는 것은 비효율적이기 때문임\n",
    "        #따라서 input의 형태는 (batch_size, seq_length)이다.\n",
    "\n",
    "        \n",
    "        # #src_lang vectorize\n",
    "        # src_slices = tf.data.Dataset.from_tensor_slices(src_lang)\n",
    "        # self.vectorizer_src.adapt(src_slices)\n",
    "        # src_lang = self.vectorizer_src(src_lang)\n",
    "\n",
    "        #vectorized된 input으로 pad_mask 생성\n",
    "        pad_mask = create_pad_mask(src_lang)\n",
    "\n",
    "        #vectorized된 input으로 embedding + positional encoding 실행\n",
    "        src_embedding = self.embedding_src(src_lang)\n",
    "        src_positionalencoding = positional_encoding(src_embedding)\n",
    "        src_embedded = src_embedding * tf.math.sqrt(tf.cast(self.dim_model, tf.float32)) + src_positionalencoding\n",
    "\n",
    "        #encoder 실행\n",
    "        out_enc = self.encoder(src_embedded, pad_mask)\n",
    "\n",
    "        # #tar_lang vectorize\n",
    "        # tar_slices = tf.data.Dataset.from_tensor_slices(tar_lang)\n",
    "        # self.vectorizer_tar.adapt(tar_slices)\n",
    "        # tar_lang = self.vectorizer_tar(tar_lang)\n",
    "\n",
    "        #vectorized된 input으로 look ahead mask 생성\n",
    "        look_ahead_mask = create_mask(tar_lang)\n",
    "\n",
    "        #vectorized된 input으로 embedding + positional encoding 실행\n",
    "        tar_embedding = self.embedding_tar(tar_lang)\n",
    "        tar_positionalencoding = positional_encoding(tar_embedding)\n",
    "        tar_embedded = tar_embedding * tf.math.sqrt(tf.cast(self.dim_model, tf.float32)) + tar_positionalencoding\n",
    "\n",
    "        #decoder 실행\n",
    "        out_dec = self.decoder(tar_embedded, out_enc, pad_mask, look_ahead_mask)\n",
    "\n",
    "        #generator 실행\n",
    "        out = self.final_layer(out_dec)\n",
    "        \n",
    "        return out        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef8da15-d80b-4e6b-9f59-6a7923822704",
   "metadata": {},
   "source": [
    "## 하이퍼파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ad76e85-f927-41ba-8fa0-f3f945b57698",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_model = 256\n",
    "num_head = 8\n",
    "dim_ff = 1028\n",
    "num_layer = 3\n",
    "max_token = 20000\n",
    "seq_len = 40\n",
    "\n",
    "# input_vocab_size = vocab_size_ko = vectorizer_ko.vocab_size + 2\n",
    "# output_vocab_size = vocab_size_en = vectorizer_en.vocab_size + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b043275b-355a-4a59-9d99-eeb85cb23c7b",
   "metadata": {},
   "source": [
    "## loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65e49d6d-2e01-4127-b285-8c9c3104cf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) #from_logits=True로 하면 Dense 이후 softmax layer 값 출력\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0)) # 예를 들어서 실제 자료(0은 패딩)가 [1,2,3,4,5,0,0,0,0,0] 이라면 [0,0,0,0,0,1,1,1,1,1]로 바꿔 줌\n",
    "                                                     # 이후 tf.math.logical_not을 활용해서 [True,True,True,True,True,False,False,False,False,False]으로 바꿔 줌\n",
    "  loss_ = loss_object(real, pred) # loss_는 패딩을 고려하지 않은 loss 값\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype) # [True,True,True,True,True,False,False,False,False,False]를 [1,1,1,1,1,0,0,0,0,0] 으로 바꿔 줌\n",
    "  loss_ *= mask # loss에 mask를 곱해서, 패딩인 부분은 0처리 해줌\n",
    "\n",
    "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b02301-3011-40a9-abad-60160db6814c",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2dec2b9d-8622-4704-aa4c-a191721be0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, dim_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "    \n",
    "    self.dim_model = dim_model\n",
    "    self.dim_model = tf.cast(self.dim_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "    \n",
    "  def __call__(self, step):\n",
    "    step = tf.cast(step, tf.float32)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "    return tf.math.rsqrt(self.dim_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93249d6c-c667-4ed1-b0b7-77e8224da972",
   "metadata": {},
   "source": [
    "## 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa349163-200d-4c1f-9829-ad11e261aa86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SID                                                 원문  \\\n",
      "0    1  'Bible Coloring'은 성경의 아름다운 이야기를 체험 할 수 있는 컬러링 ...   \n",
      "1    2                                       씨티은행에서 일하세요?   \n",
      "2    3              푸리토의 베스트셀러는 해외에서 입소문만으로 4차 완판을 기록하였다.   \n",
      "3    4   11장에서는 예수님이 이번엔 나사로를 무덤에서 불러내어 죽은 자 가운데서 살리셨습니다.   \n",
      "4    5     6.5, 7, 8 사이즈가 몇 개나 더 재입고 될지 제게 알려주시면 감사하겠습니다.   \n",
      "\n",
      "                                                 번역문  \n",
      "0  Bible Coloring' is a coloring application that...  \n",
      "1                        Do you work at a City bank?  \n",
      "2  PURITO's bestseller, which recorded 4th rough ...  \n",
      "3  In Chapter 11 Jesus called Lazarus from the to...  \n",
      "4  I would feel grateful to know how many stocks ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 엑셀 파일 경로\n",
    "file_path = 'kor_enc.xlsx'\n",
    "\n",
    "# 엑셀 파일 로드\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# 데이터 확인\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfe8a5cf-173c-4b2b-8712-a5a058b601dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 및 영어 데이터 추출\n",
    "korean_texts = df['원문'].astype(str).tolist()\n",
    "english_texts = df['번역문'].astype(str).tolist()\n",
    "\n",
    "# 한국어 텍스트 벡터화\n",
    "vectorizer_ko = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(korean_texts, target_vocab_size=max_token)\n",
    "\n",
    "# 영어 텍스트 벡터화\n",
    "vectorizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(english_texts, target_vocab_size=max_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752ebc49-e028-4c8c-a8ea-f4fabb7a4947",
   "metadata": {},
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ed712f-542a-4e97-9243-f72fda6a2b66",
   "metadata": {},
   "source": [
    "# 오래 걸리는 부분 저장하기\n",
    "with open('vectorizers.pkl', 'wb') as f:\n",
    "    pkl.dump([vectorizer_ko, vectorizer_en], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c19366-b293-4fc9-a28b-f3b945579e7d",
   "metadata": {},
   "source": [
    "# 가져다 쓰기\n",
    "with open('vectorizers.pkl', 'rb') as f:\n",
    "    vectorizer_ko, vectorizer_en = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa5f6510-2f1e-40c6-abcd-a054cb15c3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token_ko, end_token_ko = [vectorizer_ko.vocab_size], [vectorizer_ko.vocab_size + 1]\n",
    "start_token_en, end_token_en = [vectorizer_en.vocab_size], [vectorizer_en.vocab_size + 1]\n",
    "vocab_size_ko = vectorizer_ko.vocab_size + 2\n",
    "vocab_size_en = vectorizer_en.vocab_size + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c68ac8cb-f032-4fdf-a14d-d389599ae740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시작 토큰 번호 : [19708] [20012]\n",
      "종료 토큰 번호 : [19709] [20013]\n",
      "단어 집합의 크기 : 19710\n",
      "단어 집합의 크기 : 20014\n"
     ]
    }
   ],
   "source": [
    "print('시작 토큰 번호 :',start_token_ko, start_token_en)\n",
    "print('종료 토큰 번호 :',end_token_ko, end_token_en)\n",
    "print('단어 집합의 크기 :',vocab_size_ko)\n",
    "print('단어 집합의 크기 :',vocab_size_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73271cb9-d83b-4f46-900b-527906506987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 길이를 40으로 정의\n",
    "seq_length = seq_len\n",
    "\n",
    "# 토큰화 / 정수 인코딩 / 시작 토큰과 종료 토큰 추가 / 패딩\n",
    "def vectorize_and_filter(src, tar):\n",
    "  vectorized_src, vectorized_tar = [], []\n",
    "  \n",
    "  for (sentence_ko, sentence_en) in zip(src, tar):\n",
    "    # encode(토큰화 + 정수 인코딩), 시작 토큰과 종료 토큰 추가\n",
    "    sentence_ko = start_token_ko + vectorizer_ko.encode(sentence_ko) + end_token_ko\n",
    "    sentence_en = start_token_en + vectorizer_en.encode(sentence_en) + end_token_en\n",
    "      \n",
    "    if len(sentence_ko) <= seq_length and len(sentence_en) <= seq_length:\n",
    "      vectorized_src.append(sentence_ko)\n",
    "      vectorized_tar.append(sentence_en)\n",
    "  \n",
    "  # 패딩\n",
    "  vectorized_src = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      vectorized_src, maxlen=seq_length, padding='post')\n",
    "  vectorized_tar = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      vectorized_tar, maxlen=seq_length, padding='post')\n",
    "  \n",
    "  return vectorized_src, vectorized_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "962ec049-19d8-4879-b7ff-598cbe947da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_text, en_text = vectorize_and_filter(korean_texts, english_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ecd4a9c-c3cc-44db-af10-15703811ee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, validation, test set 나누기\n",
    "korean_train, korean_val, korean_test = ko_text[:180000], ko_text[180000:195000], ko_text[195000:]\n",
    "english_train, english_val, english_test = en_text[:180000], en_text[180000:195000], en_text[195000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c068684a-1169-43d0-8f7d-bf6d6ca3b9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "buffer_size = len(en_text)\n",
    "\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((korean_train, english_train)).batch(batch_size).shuffle(buffer_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "dataset_val = tf.data.Dataset.from_tensor_slices((korean_val, english_val)).batch(batch_size).shuffle(buffer_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((korean_test, english_test)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9bbe95f-8d23-416e-b14b-521076634eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(dim_model=dim_model,num_head=num_head,dim_ff=dim_ff,num_layer=num_layer,input_vocab_size=vocab_size_ko,output_vocab_size=vocab_size_en,seq_len=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0ccbefb-4811-4192-8f8b-7edbd636a794",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(dim_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  # ensure labels have shape (batch_size, MAX_LENGTH - 1)\n",
    "  y_true = tf.reshape(y_true, shape=(-1, seq_len - 1))\n",
    "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "transformer.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "469763ca-9863-46d4-97d7-b9293aa77ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인풋, 아웃풋의 텐셔 shape 정의\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "]\n",
    "\n",
    "# tf.function을 사용하면 그래프를 미리 컴파일 하기 때문에 속도가 상당히 빠름\n",
    "# 같은 GPU여도 케라스에 비해서 체감상 7~8배 정도의 차이가 나는 것 같음\n",
    "# @tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "  tar_inp = tar[:, :-1]\n",
    "  tar_real = tar[:, 1:]\n",
    "  \n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = transformer(inp, tar_inp)\n",
    "    loss = loss_function(tar_real, predictions)\n",
    "\n",
    "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "  train_loss(loss)\n",
    "  train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4af7cd6d-c1d9-44f8-8d5c-db53198717d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!!\n"
     ]
    }
   ],
   "source": [
    "# 저장할 체크포인트 지정\n",
    "checkpoint_path = \"./\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2762bac4-2fec-49f9-b01b-75e5f81b1272",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import time\n",
    "# 5 에포크 훈련\n",
    "for epoch in range(5):\n",
    "  start = time.time()\n",
    "  \n",
    "  train_loss.reset_state()\n",
    "  train_accuracy.reset_state()\n",
    "  \n",
    "  # input : 한국어, tar : 영어\n",
    "  for (batch, (inp, tar)) in enumerate(dataset_train):\n",
    "    train_step(inp, tar)\n",
    "    \n",
    "    if batch % 50 == 0:\n",
    "      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "      \n",
    "  if (epoch + 1) % 5 == 0:\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                         ckpt_save_path))\n",
    "    \n",
    "  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "\n",
    "  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "507cf91a-2f48-449a-8b19-3a6417d58370",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator():\n",
    "    def __init__(self, transformer, vectorizer_ko, vectorizer_en):\n",
    "        self.transformer = transformer\n",
    "        self.vectorizer_ko = vectorizer_ko\n",
    "        self.vectorizer_en = vectorizer_en\n",
    "\n",
    "    def __call__(self, sentence, seq_length):\n",
    "        self.start_en = self.vectorizer_en.vocab_size\n",
    "        self.end_en = self.vectorizer_en.vocab_size + 1\n",
    "        self.new_vocab = [vectorizer_en.decode([i]) for i in range(vectorizer_en.vocab_size)] + ['<start>', '<end>']\n",
    "        self.new_vectorizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(self.new_vocab, target_vocab_size=vectorizer_en.vocab_size + 2)\n",
    "        self.sentence = tf.expand_dims(self.vectorizer_ko.encode(sentence), axis=0)  # 차원 추가\n",
    "        \n",
    "        output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "        output_array = output_array.write(0, [self.start_en])\n",
    "\n",
    "        for i in tf.range(seq_length):\n",
    "            output = tf.transpose(output_array.stack())\n",
    "            predictions = self.transformer(self.sentence, output)\n",
    "\n",
    "            predictions = predictions[:, -1:, :]  \n",
    "            predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "            output_array = output_array.write(i + 1, predicted_id[0])\n",
    "\n",
    "            if tf.reduce_all(predicted_id[0][0] == self.end_en):\n",
    "                break\n",
    "\n",
    "        output = tf.transpose(output_array.stack())\n",
    "        final_output = self.new_vectorizer_en.decode(tf.squeeze(output))\n",
    "\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc167a15-f455-4d82-865d-ba926b4b1a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator(transformer, vectorizer_ko, vectorizer_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb970b4f-8cbe-4178-b21f-db5c5ea4159b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"나는 한국인이다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74cdb55c-b58f-4f45-a88b-260542c9adaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'�start℃yourselfz +�'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translator(sentence, 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
