{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f33d43b6-b663-4fc8-9cc2-5829f9e75198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5eae50-8474-4f2c-a0db-e0c65efe5fab",
   "metadata": {},
   "source": [
    "## 구조 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7151cd34-d196-461e-b3b8-338d2a29b39a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTransformer(num_layers, input_src, input_tgt, max_vocab, seq_length, dim_embedding, num_head, dff)\\n    Encoder(num_layers, input_src, max_vacab, seq_length, dim_embedding, pad_mask, num_head, dff)\\n        Encoder layer(input_src, max_vacab, seq_length, dim_embedding, pad_mask, num_head, dff)\\n            vectorization(input_src, max_vacab, seq_length)\\n            embedding(input_src2, max_vacab, dim_embedding)\\n            positional encoding(input_src3, position=seq_length, depth=dim_embedding)\\n            multi-head self attention(input_src4, dim_model=dim_embedding, num_head)\\n                pad_mask\\n            add&normalization(input_src5)\\n            feed forward neural network(input_src6, dim_model, dff)\\n            add&normalization(input_src7)\\n    Decoder(num_layers, num_layers, input, max_vacab, seq_length, dim_embedding, pad_mask, num_head, dff)\\n        Decoder layer(input_tgt, max_vacab, seq_length, dim_embedding, pad_mask, num_head, dff, enc_out)\\n            vectorization(input_tgt, max_vacab, seq_length)\\n            embedding(input_tgt2, max_vacab, dim_embedding)\\n            positional encoding(input_tgt3, position=seq_length, depth=dim_embedding)\\n            masked multi-head self attention(input_tgt4, dim_model=dim_embedding, num_head)\\n                mask, pad_mask\\n            add&normalization(input_tgt5)\\n            cross multi-head self attention(input_tgt6, enc_output, pad_mask, dim_model=dim_embedding, num_head)\\n            add&normalization(input_tgt7)\\n            feed forward neural network(input_tgt8, dim_model, dff)\\n            add&normalization(input_tgt9)\\n    Generator\\n        Linear(dec_out)\\n        Softmax(linear_out)\\n            \\n'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Transformer(num_layers, input_src, input_tgt, max_vocab, seq_length, dim_embedding, num_head, dff)\n",
    "    Encoder(num_layers, input_src, max_vacab, seq_length, dim_embedding, pad_mask, num_head, dff)\n",
    "        Encoder layer(input_src, max_vacab, seq_length, dim_embedding, pad_mask, num_head, dff)\n",
    "            vectorization(input_src, max_vacab, seq_length)\n",
    "            embedding(input_src2, max_vacab, dim_embedding)\n",
    "            positional encoding(input_src3, position=seq_length, depth=dim_embedding)\n",
    "            multi-head self attention(input_src4, dim_model=dim_embedding, num_head)\n",
    "                pad_mask\n",
    "            add&normalization(input_src5)\n",
    "            feed forward neural network(input_src6, dim_model, dff)\n",
    "            add&normalization(input_src7)\n",
    "    Decoder(num_layers, num_layers, input, max_vacab, seq_length, dim_embedding, pad_mask, num_head, dff)\n",
    "        Decoder layer(input_tgt, max_vacab, seq_length, dim_embedding, pad_mask, num_head, dff, enc_out)\n",
    "            vectorization(input_tgt, max_vacab, seq_length)\n",
    "            embedding(input_tgt2, max_vacab, dim_embedding)\n",
    "            positional encoding(input_tgt3, position=seq_length, depth=dim_embedding)\n",
    "            masked multi-head self attention(input_tgt4, dim_model=dim_embedding, num_head)\n",
    "                mask, pad_mask\n",
    "            add&normalization(input_tgt5)\n",
    "            cross multi-head self attention(input_tgt6, enc_output, pad_mask, dim_model=dim_embedding, num_head)\n",
    "            add&normalization(input_tgt7)\n",
    "            feed forward neural network(input_tgt8, dim_model, dff)\n",
    "            add&normalization(input_tgt9)\n",
    "    Generator\n",
    "        Linear(dec_out)\n",
    "        Softmax(linear_out)\n",
    "            \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5386bc1e-41ec-43a8-9a54-91ac3d0b6d67",
   "metadata": {},
   "source": [
    "## Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "dd749dfe-9726-4e5b-b17d-7a219b3f6ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(inputs):\n",
    "    #input의 형태는 문장을 vectorization -> embedding한 (batch_size, seq_length, dim_embedding)의 형태로 입력이 됨\n",
    "    #batch_size는 모델의 파라미터로 한번에 처리할 문장의 갯수\n",
    "    #seq_length는 한 문장에서 처리할 Token의 숫자\n",
    "    #dim_embedding은 한개의 token을 몇 차원으로 embedding하는지 나타내는 값\n",
    "    seq_length = inputs.shape[-2]\n",
    "    dim_embedding = inputs.shape[-1]\n",
    "    positional_encoding = np.zeros((seq_length, dim_embedding))\n",
    "\n",
    "    \n",
    "    #(seq_length, dim_embedding)의 구조로 된 문장을 positional encoding을 하기 위해 pos(행), i(열) 좌표(?) 위치를 계산\n",
    "    pos = np.arange(seq_length)[:,np.newaxis] #행의 번호\n",
    "    i = np.arange(dim_embedding)[np.newaxis,:] #열의 번호\n",
    "    d_model = dim_embedding\n",
    "\n",
    "    #positional encoding angle 공식\n",
    "    angle = pos*1/np.power(10000,(2 * (i // 2) / np.float32(dim_embedding)))\n",
    "\n",
    "    #i가 짝수 일때는 sin, i가 홀수 일때는 cos으로 계산하여, 위치별로 더해줄 positional encoding 값을 계산\n",
    "    positional_encoding[:, 0::2] = np.sin(angle[:, 0::2])\n",
    "    positional_encoding[:, 1::2] = np.cos(angle[:, 1::2])\n",
    "    positional_encoding[tf.newaxis, ...] #(batch_size, seq_length, dim_embedding)형태로 만들어 주기 위해 batch_size 부분에 해당하는 np.newaxis 추가\n",
    "\n",
    "    return tf.cast(positional_encoding, dtype=tf.float32) #output 차원 = (batch_size, seq_length, dim_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5687ee0-8020-4436-bda9-8562a07da3a3",
   "metadata": {},
   "source": [
    "## scaled_dot_product & split_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d012ecb6-c5be-420a-8305-c1cd8e516365",
   "metadata": {},
   "outputs": [],
   "source": [
    "#self attention에서는 input으로 embedded positional encoded sentence를 받고 query, key, value를 만들어 self attention score를 얻어낸다.\n",
    "#필요한 input: input\n",
    "\n",
    "#input을 받아서 query, key, value를 만든다.\n",
    "#matmul : matmul(query,key.transpose)\n",
    "#scaling : matmul/tf.sqrt(dim_k)\n",
    "#pad_mask : scaled + pad_mask\n",
    "#softmax : softmax(pad_masked)\n",
    "#output : matmul(softmax, value)\n",
    "\n",
    "def create_pad_mask(x):\n",
    "    #여기서 input 값인 x는 vectorize된 문장이 들어가야 함\n",
    "    #input 형태 : (batch_size, seq_length) -> [[1,2,3,1,0,0,0],[3,2,4,0,0,0,0]] 이런 형태로 입력\n",
    "    #matirx에서 x값이 0과 같은 경우, True, 아니면 False를 반환하고, tf.cast를 통해서 boolen을 float32로 변환(True = 1.0, False=0.0)\n",
    "    mask = tf.cast(tf.math.equal(x, 0), dtype=tf.float32)\n",
    "    #return은 multi-head self attention에서 사용하기 위해 (batch_size, num_head, seq_length, seq_length)의 형태로 만들어줌\n",
    "    #batch_size는 문장의 숫자가 되고, num_head는 multi-head를 사용할 때 필요한 차원이고, scaled_dot_product에서 mask를 적용해야 할 matmul_qk의 차원이 seq_length, seq_length인데 우리는 Column에다가 mask를 적용할 것이기 떄문에, (Batch_size, 1, 1, seq_length)의 차원으로 만들어줌\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :] #output 차원 (batch_size, 1, 1, seq_length)\n",
    "\n",
    "\n",
    "def scaled_dot_product(query, key, value, pad_mask=None):\n",
    "    # query와 key의 transpose dot product이후 scaling\n",
    "    # 입력 query, key, value의 차원 = (batch_size, seq_length, dim_k)\n",
    "    # 즉 입력은 Embedding layer와 Positional encoding layer를 통과한 후의 데이터가 입력 되면 됨\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b = True) #차원 = (batch_szie, seq_length, seq_length)\n",
    "    dim_k = tf.cast(key.shape[-1], tf.float32)\n",
    "    scaled_matmul_qk = matmul_qk / tf.math.sqrt(tf.cast(dim_k, tf.float32)) #차원 = (batch_size, seq_length, seq_length)\n",
    "\n",
    "    if pad_mask is not None:\n",
    "        # 패딩된 위치에 -1e9를 더해 softmax에서 0에 가까운 값이 되도록 함 \n",
    "        # pad mask의 차원은 (batch_size, num_head, seq_length, seq_length)이므로, multi_head self attention에서 사용해야함\n",
    "        scaled_matmul_qk += (pad_mask * -1e9) #차원은 (batch_size, (num_head), seq_length, seq_length)\n",
    "    \n",
    "    # 소프트맥스 적용\n",
    "    softmax_qk = tf.nn.softmax(scaled_matmul_qk, axis=-1) #행 방향으로 softmax를 하고, 차원은 변경 없이 (batch_size, (num_head), seq_length, seq_length)\n",
    "    \n",
    "    # Attention 결과 계산\n",
    "    attention = tf.matmul(softmax_qk, value) #차원은 (batch_size, (num_head), seq_length, dim_k)\n",
    "    \n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "15478aa3-be3f-4728-850b-59dab8ed0ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_heads(x, batch_size, num_head, depth):\n",
    "    #multi-head self attention을 하기 원래 query, key, value(batch_size, seq_length, dim_k)를 num_haed만큼 concat을 시켜서 최종 attention을 얻어야 한다.\n",
    "    #이를 병렬 처리 하기 위해서 query, key, value를 (batch_size, seq_length, num_head, depth(뒤에서는 dim_k=dim_model/num_head로 표현))의 형태로 한번에 변환을 해줌\n",
    "    #reshape과정에서 데이터가 바뀌면 안되기 때문에, 원래의 query (batch_size, seq_length, dim_model)에서 dim_model을 num_head로 잘라내고 남은 depth를 마지막 차원에 둠, -1을 넣는 것은 seq_length 자리임, 자동 계산되고 남은 숫자가 seq_length에 들어가게 됨\n",
    "    #마지막으로 병렬처리르 할 수 있게 (batch, seq_length, num_head, depth)인 데이터를 (batch_size, num_head, seq_length, depth)로 변경해서, num_head만큼 동시에 병렬처리를 진행\n",
    "    x = tf.reshape(x, (batch_size, -1, num_head, depth))\n",
    "    return tf.transpose(x, perm=[0,2,1,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4759342-06aa-4e23-b70d-a22835bde8b3",
   "metadata": {},
   "source": [
    "## multi-head self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8617a61c-dfe8-47b3-9922-67ed3a7cf4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiheadselfattention():\n",
    "    def __init__(self, dim_model, num_head):\n",
    "        self.dim_model = dim_model\n",
    "        self.num_head = num_head\n",
    "        self.depth = dim_model // num_head\n",
    "\n",
    "        # query, key, value에 Dense layer를 생성\n",
    "        self.q_layer = tf.keras.layers.Dense(dim_model)\n",
    "        self.k_layer = tf.keras.layers.Dense(dim_model)\n",
    "        self.v_layer = tf.keras.layers.Dense(dim_model)\n",
    "\n",
    "        # 최종 계산된 attention output을 입력값 차원과 동일하게 dim_embedding으로 바꿔주기 위해서 Dense_layer 사용\n",
    "        self.dim_embedding = dim_model\n",
    "        self.dense_layer = tf.keras.layers.Dense(self.dim_embedding)\n",
    "            \n",
    "    def __call__(self, query, key, value, pad_mask):\n",
    "        #batch_size를 입력된 data를 통해서 확인\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        #위에서 만든 각각의 Dense layer를 이용하여 query, key, value 계산\n",
    "        q = self.q_layer(query)\n",
    "        k = self.k_layer(key)\n",
    "        v = self.v_layer(value)\n",
    "\n",
    "        #한번에 계산이된 dim_model 차원의 query, key, value 값을 위에서 정의한 split_head 함수로 num_head개의 depth 차원으로 분할\n",
    "        q = split_heads(q, batch_size, self.num_head, self.depth)\n",
    "        k = split_heads(k, batch_size, self.num_head, self.depth)\n",
    "        v = split_heads(v, batch_size, self.num_head, self.depth)\n",
    "\n",
    "        #위에서 정의한 scaled_dot_product에 q, k, v, 그리고 input data에 맞게 생성된 pad_mask를 입력하여 self attention 연산\n",
    "        attention = scaled_dot_product(q, k, v, pad_mask) #입력된 값은 (batch_size, num_head, seq_length, depth)차원이고, output역시 동일하게 (batch_size, num_head, seq_length, depth)가 된다.\n",
    "        attention_scaled = tf.transpose(attention, perm=[0,2,1,3]) #위의 output이 (batch_size, num_head, seq_length, depth)이기 떄문에 다시 concat을 하고 원래 최초에 들어온 input인 (batch_size, seq_length, dim_model) 차원을 맞추기 위해서 (batch_size, seq_length, num_head, depth)로 차원을 바꿔준다.\n",
    "        attention_concat = tf.reshape(attention_scaled, (batch_size, -1, self.dim_model)) #위에서 바꾼 차원을 값을 num_head와 depth를 합쳐 (batch_size, seq_length, dim_model로 바꿔줌\n",
    "        attention_output = self.dense_layer(attention_concat)\n",
    "\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2c1e2023-769b-4660-8cfd-3dab438db776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_sample(batch_size=32, seq_length=40, embedding_dim=512):\n",
    "    \"\"\"\n",
    "    임베딩된 텍스트를 시뮬레이션하는 샘플 데이터를 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "    - batch_size: 배치 크기\n",
    "    - seq_length: 시퀀스 길이\n",
    "    - embedding_dim: 임베딩 차원\n",
    "    \n",
    "    Returns:\n",
    "    - 생성된 샘플 데이터 (shape: [batch_size, seq_length, embedding_dim])\n",
    "    \"\"\"\n",
    "    # 정규 분포를 사용하여 랜덤한 임베딩 생성\n",
    "    sample_data = tf.random.normal(shape=[batch_size, seq_length, embedding_dim])\n",
    "    \n",
    "    return sample_data\n",
    "\n",
    "# 샘플 데이터 생성\n",
    "sample_embeddings = create_embedding_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eba42ca-1463-4708-b4b9-cbcf169800f8",
   "metadata": {},
   "source": [
    "## Fead Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "49ae6f70-68da-4225-bfcf-c8934794fe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feadforwardneuralnetwork():\n",
    "    def __init__(self, dim_model, dim_ff):\n",
    "        self.dim_ff = dim_ff\n",
    "        self.dim_model = dim_model\n",
    "        self.dense_layer1 = tf.keras.layers.Dense(self.dim_ff, activation='relu')\n",
    "        self.dense_layer2 = tf.keras.layers.Dense(self.dim_model)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        dense_layer2 = tf.keras.layers.Dense(self.dim_model)\n",
    "        x1 = self.dense_layer1(x)\n",
    "        output_ffnn = self.dense_layer2(x1)\n",
    "        return output_ffnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0271eca8-17ac-4f9f-9dbc-c133fbc83b30",
   "metadata": {},
   "source": [
    "## Encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2afd4a4e-0c2c-456d-b30c-11edf2b80ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoderlayer():\n",
    "    def __init__(self, dim_model, num_head, dim_ff):\n",
    "        self.dim_model = dim_model\n",
    "        self.num_head = num_head\n",
    "        self.dim_ff = dim_ff\n",
    "        \n",
    "        self.mha = Multiheadselfattention(self.dim_model, self.num_head)\n",
    "        self.ffnn = Feadforwardneuralnetwork(self.dim_model, self.dim_ff)\n",
    "\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def __call__(self, x, pad_mask):\n",
    "        attention = self.mha(x, x, x, pad_mask)\n",
    "\n",
    "        attention_norm = self.norm1(attention+x)\n",
    "\n",
    "        output_ffnn = self.ffnn(attention_norm)\n",
    "\n",
    "        output_ffnn_norm = self.norm2(output_ffnn+attention_norm)\n",
    "\n",
    "        return output_ffnn_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05f661b-78b9-4508-9e8a-116f8d048322",
   "metadata": {},
   "source": [
    "## look ahead mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ef977c7e-2c20-42f3-b5dd-ca3f1185f20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookaheadmask(seq_length):\n",
    "    #tf.ones((seq_length, seq_length))는 seq_length*seq_length의 matrix에 1을 가득 채운 것임\n",
    "    #tf.linalg.band_part(matrix, num_lower, num_upper)에서 num_lower는 대각선 아래 유지할 행의 개수, num_upper는 대각선 위 유지할 열의 개수이고, -1을 입력하면 전체를 의미하고, 유지하지 않는 부분은 0으로 채움\n",
    "    #1-tf.linalg~~~ 를 진행하는 이유는 나중에 1e-9를 곱해주어서 softmax에서 0을 만들기 위함\n",
    "    mask = 1-tf.linalg.band_part(tf.ones((seq_length, seq_length)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "def create_mask(x):\n",
    "    #padding과 look ahead mask를 한번에 만들어주는 함수\n",
    "    #input으로 사용되는 x는 vectorized된 문장이다. \n",
    "    #input 형태 : (batch_size, seq_length) -> [[1,2,3,1,0,0,0],[3,2,4,0,0,0,0]] 이런 형태로 입력\n",
    "    temp_mask = create_lookaheadmask(tf.shape(x)[1]) #temp_mask는 위의 create_lookaheadmask를 그대로 실행해서 만듬\n",
    "    reverse_tar = tf.cast(tf.math.equal(x, 0),dtype=tf.float32) #입력이 되는 tar 문장의 pad mask이다. 즉 pad 부분을 1로, 단어가 있는 부분을 0으로 만드는 식\n",
    "    reverse_tar = reverse_tar[:,tf.newaxis,tf.newaxis,:] #나중에 (batch_size, num_head, seq_length, seq_length) 형태로 더해주기 위해 형태 변형 -> (batch_size, 1, 1, seq_length)형태\n",
    "    look_ahead_mask = tf.maximum(reverse_tar, temp_mask) #최종형태는 pad와 look_ahead 부분을 모두 가리는 mask\n",
    "    return look_ahead_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57511d61-bf12-4d3f-91bd-46247dfe8a5d",
   "metadata": {},
   "source": [
    "## Decoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e7340831-e728-463c-8858-557434bd8eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_layer():\n",
    "    def __init__(self, dim_model, num_head, dim_ff):\n",
    "        self.dim_model = dim_model\n",
    "        self.dim_ff = dim_ff\n",
    "        self.num_head = num_head\n",
    "\n",
    "        self.mha1 = Multiheadselfattention(self.dim_model, self.num_head)\n",
    "        self.mha2 = Multiheadselfattention(self.dim_model, self.num_head)\n",
    "        self.ffnn = Feadforwardneuralnetwork(self.dim_model, self.dim_ff)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "    def __call__(self, tar, out_enc, pad_mask, look_ahead_mask):        \n",
    "        attention = self.mha1(tar, tar, tar, look_ahead_mask)\n",
    "\n",
    "        attention_norm = self.norm1(attention+tar)\n",
    "\n",
    "        cross_attention = self.mha2(attention_norm, out_enc, out_enc, pad_mask)\n",
    "\n",
    "        cross_attention_norm = self.norm2(cross_attention + attention_norm)\n",
    "\n",
    "        ffnn = self.ffnn(cross_attention_norm)\n",
    "\n",
    "        ffnn_norm = self.norm3(ffnn+cross_attention_norm)\n",
    "\n",
    "        return ffnn_norm       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0e072d-be4e-4d0c-9eee-0913cf70336e",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "659844a1-2020-420c-902d-465a9125994f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder():\n",
    "    def __init__(self, dim_model, num_head, dim_ff, num_layer):\n",
    "        self.dim_model = dim_model\n",
    "        self.num_head = num_head\n",
    "        self.dim_ff = dim_ff\n",
    "        self.num_layer = num_layer\n",
    "\n",
    "        self.encoder_block = [Encoderlayer(self.dim_model,self.num_head,self.dim_ff) for _ in range(self.num_layer)]\n",
    "\n",
    "    def __call__(self, x, pad_mask):\n",
    "        for i in range(self.num_layer):\n",
    "            x = self.encoder_block[i](x, pad_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dc3c19-186f-4cea-b6c9-c8fb5548682a",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "46a4b044-f66b-42bf-9ea1-258cac99fff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder():\n",
    "    def __init__(self, dim_model, num_head, dim_ff, num_layer):\n",
    "        self.dim_model = dim_model\n",
    "        self.num_head = num_head\n",
    "        self.dim_ff = dim_ff\n",
    "        self.num_layer = num_layer\n",
    "\n",
    "        self.decoder_block = [Decoder_layer(self.dim_model,self.num_head,self.dim_ff) for _ in range(self.num_layer)]\n",
    "\n",
    "    def __call__(self, tar, out_enc, pad_mask, look_ahead_mask):\n",
    "        for i in range(self.num_layer):\n",
    "            out = self.decoder_block[i](tar,out_enc, pad_mask, look_ahead_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6878a896-6549-4b7e-923a-cafe2e2d5d39",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "16acf24d-8577-45e8-9643-a17a957b63a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    #Transforemr에서 사용하는 Parameter는 여러군데에서 사용이 되어서 혼동이 될 수 있는데 아래를 참고하자.\n",
    "    #dim_model : dim_model 자체는 Multi-head attention에서 사용을 하는 dimension인데, input = output을 유지하기 위해서, dim_embedding, depth*num_head와 동일한 값을 가진다.\n",
    "    #num_head : multi-head에서 몇 개의 head로 병렬 연산을 할 것인지 결정하는 것이다.\n",
    "    #dim_ff : ffnn의 Dense layer에서 사용되는 dimension으로 특정되는 숫자가 있지는 않다.\n",
    "    #num_layer : encoder와 decoder layer들의 반복 횟수이다.\n",
    "    #max_token : vectorize할 때 최대로 사용할 token의 숫자이고, embedding할 때도 동일하게 가져간다.\n",
    "    #seq_len : 한 문장 안에서 input으로 처리할 최대의 단어 숫자를 의미\n",
    "    def __init__(self, dim_model, num_head, dim_ff, num_layer, input_vocab_size, output_vocab_size, seq_len):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.dim_model = dim_model\n",
    "        self.num_head = num_head\n",
    "        self.dim_ff = dim_ff\n",
    "        self.num_layer = num_layer\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.encoder = Encoder(self.dim_model, self.num_head, self.dim_ff, self.num_layer)\n",
    "        self.decoder = Decoder(self.dim_model, self.num_head, self.dim_ff, self.num_layer)\n",
    "        # self.vectorizer_src = tf.keras.layers.TextVectorization(max_tokens=self.max_token,output_mode='int',output_sequence_length=self.seq_len)\n",
    "        # self.vectorizer_tar = tf.keras.layers.TextVectorization(max_tokens=self.max_token,output_mode='int',output_sequence_length=self.seq_len)\n",
    "        self.embedding_src = tf.keras.layers.Embedding(input_dim=self.input_vocab_size, output_dim=self.dim_model)\n",
    "        self.embedding_tar = tf.keras.layers.Embedding(input_dim=self.output_vocab_size, output_dim=self.dim_model)\n",
    "        self.final_layer = tf.keras.layers.Dense(self.output_vocab_size)\n",
    "\n",
    "    def __call__(self, src_lang, tar_lang):\n",
    "        #src_lang, tar_lang은 vectorized된 input을 받는게 좋다. 매번 실행할 때마다, vocab을 학습하고, vectorize를 하는 것은 비효율적이기 때문임\n",
    "        #따라서 input의 형태는 (batch_size, seq_length)이다.\n",
    "\n",
    "        \n",
    "        # #src_lang vectorize\n",
    "        # src_slices = tf.data.Dataset.from_tensor_slices(src_lang)\n",
    "        # self.vectorizer_src.adapt(src_slices)\n",
    "        # src_lang = self.vectorizer_src(src_lang)\n",
    "\n",
    "        #vectorized된 input으로 pad_mask 생성\n",
    "        pad_mask = create_pad_mask(src_lang)\n",
    "\n",
    "        #vectorized된 input으로 embedding + positional encoding 실행\n",
    "        src_embedding = self.embedding_src(src_lang)\n",
    "        src_positionalencoding = positional_encoding(src_embedding)\n",
    "        src_embedded = src_embedding * tf.math.sqrt(tf.cast(self.dim_model, tf.float32)) + src_positionalencoding\n",
    "\n",
    "        #encoder 실행\n",
    "        out_enc = self.encoder(src_embedded, pad_mask)\n",
    "\n",
    "        # #tar_lang vectorize\n",
    "        # tar_slices = tf.data.Dataset.from_tensor_slices(tar_lang)\n",
    "        # self.vectorizer_tar.adapt(tar_slices)\n",
    "        # tar_lang = self.vectorizer_tar(tar_lang)\n",
    "\n",
    "        #vectorized된 input으로 look ahead mask 생성\n",
    "        look_ahead_mask = create_mask(tar_lang)\n",
    "\n",
    "        #vectorized된 input으로 embedding + positional encoding 실행\n",
    "        tar_embedding = self.embedding_tar(tar_lang)\n",
    "        tar_positionalencoding = positional_encoding(tar_embedding)\n",
    "        tar_embedded = tar_embedding * tf.math.sqrt(tf.cast(self.dim_model, tf.float32)) + tar_positionalencoding\n",
    "\n",
    "        #decoder 실행\n",
    "        out_dec = self.decoder(tar_embedded, out_enc, pad_mask, look_ahead_mask)\n",
    "\n",
    "        #generator 실행\n",
    "        out = self.final_layer(out_dec)\n",
    "        \n",
    "        return out        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef8da15-d80b-4e6b-9f59-6a7923822704",
   "metadata": {},
   "source": [
    "## 하이퍼파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1ad76e85-f927-41ba-8fa0-f3f945b57698",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_model = 256\n",
    "num_head = 8\n",
    "dim_ff = 1028\n",
    "num_layer = 3\n",
    "max_token = 20000\n",
    "seq_len = 40\n",
    "\n",
    "# input_vocab_size = vocab_size_ko = vectorizer_ko.vocab_size + 2\n",
    "# output_vocab_size = vocab_size_en = vectorizer_en.vocab_size + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b043275b-355a-4a59-9d99-eeb85cb23c7b",
   "metadata": {},
   "source": [
    "## loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "65e49d6d-2e01-4127-b285-8c9c3104cf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) #from_logits=True로 하면 Dense 이후 softmax layer 값 출력\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0)) # 예를 들어서 실제 자료(0은 패딩)가 [1,2,3,4,5,0,0,0,0,0] 이라면 [0,0,0,0,0,1,1,1,1,1]로 바꿔 줌\n",
    "                                                     # 이후 tf.math.logical_not을 활용해서 [True,True,True,True,True,False,False,False,False,False]으로 바꿔 줌\n",
    "  loss_ = loss_object(real, pred) # loss_는 패딩을 고려하지 않은 loss 값\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype) # [True,True,True,True,True,False,False,False,False,False]를 [1,1,1,1,1,0,0,0,0,0] 으로 바꿔 줌\n",
    "  loss_ *= mask # loss에 mask를 곱해서, 패딩인 부분은 0처리 해줌\n",
    "\n",
    "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b02301-3011-40a9-abad-60160db6814c",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2dec2b9d-8622-4704-aa4c-a191721be0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, dim_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "    \n",
    "    self.dim_model = dim_model\n",
    "    self.dim_model = tf.cast(self.dim_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "    \n",
    "  def __call__(self, step):\n",
    "    step = tf.cast(step, tf.float32)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    \n",
    "    return tf.math.rsqrt(self.dim_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93249d6c-c667-4ed1-b0b7-77e8224da972",
   "metadata": {},
   "source": [
    "## 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "aa349163-200d-4c1f-9829-ad11e261aa86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   SID                                                 원문  \\\n",
      "0    1  'Bible Coloring'은 성경의 아름다운 이야기를 체험 할 수 있는 컬러링 ...   \n",
      "1    2                                       씨티은행에서 일하세요?   \n",
      "2    3              푸리토의 베스트셀러는 해외에서 입소문만으로 4차 완판을 기록하였다.   \n",
      "3    4   11장에서는 예수님이 이번엔 나사로를 무덤에서 불러내어 죽은 자 가운데서 살리셨습니다.   \n",
      "4    5     6.5, 7, 8 사이즈가 몇 개나 더 재입고 될지 제게 알려주시면 감사하겠습니다.   \n",
      "\n",
      "                                                 번역문  \n",
      "0  Bible Coloring' is a coloring application that...  \n",
      "1                        Do you work at a City bank?  \n",
      "2  PURITO's bestseller, which recorded 4th rough ...  \n",
      "3  In Chapter 11 Jesus called Lazarus from the to...  \n",
      "4  I would feel grateful to know how many stocks ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 엑셀 파일 경로\n",
    "file_path = 'kor_enc.xlsx'\n",
    "\n",
    "# 엑셀 파일 로드\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# 데이터 확인\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bfe8a5cf-173c-4b2b-8712-a5a058b601dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m english_texts \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m번역문\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 한국어 텍스트 벡터화\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m vectorizer_ko \u001b[38;5;241m=\u001b[39m \u001b[43mtfds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeprecated\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSubwordTextEncoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_from_corpus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkorean_texts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_vocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 영어 텍스트 벡터화\u001b[39;00m\n\u001b[1;32m      9\u001b[0m vectorizer_en \u001b[38;5;241m=\u001b[39m tfds\u001b[38;5;241m.\u001b[39mdeprecated\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mSubwordTextEncoder\u001b[38;5;241m.\u001b[39mbuild_from_corpus(english_texts, target_vocab_size\u001b[38;5;241m=\u001b[39mmax_token)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/tensorflow_datasets/core/deprecated/text/subword_text_encoder.py:348\u001b[0m, in \u001b[0;36mSubwordTextEncoder.build_from_corpus\u001b[0;34m(cls, corpus_generator, target_vocab_size, max_subword_length, max_corpus_chars, reserved_tokens)\u001b[0m\n\u001b[1;32m    345\u001b[0m max_token_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(token_counts\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    347\u001b[0m \u001b[38;5;66;03m# Another option could be to do a binary search over *ranks* of the tokens.\u001b[39;00m\n\u001b[0;32m--> 348\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_binary_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmin_token_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_token_count\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/tensorflow_datasets/core/deprecated/text/subword_text_encoder.py:308\u001b[0m, in \u001b[0;36mSubwordTextEncoder.build_from_corpus.<locals>._binary_search\u001b[0;34m(min_token_count, max_token_count)\u001b[0m\n\u001b[1;32m    304\u001b[0m candidate_min \u001b[38;5;241m=\u001b[39m (min_token_count \u001b[38;5;241m+\u001b[39m max_token_count) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    305\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    306\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSubwordTextEncoder build: trying min_token_count \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, candidate_min\n\u001b[1;32m    307\u001b[0m )\n\u001b[0;32m--> 308\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_from_token_counts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_counts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_counts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_token_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcandidate_min\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreserved_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreserved_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_subword_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_subword_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m vocab_size \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# Being within 1% of the target vocab size is ok\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/tensorflow_datasets/core/deprecated/text/subword_text_encoder.py:367\u001b[0m, in \u001b[0;36mSubwordTextEncoder._build_from_token_counts\u001b[0;34m(cls, token_counts, min_token_count, reserved_tokens, num_iterations, max_subword_length)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token, count \u001b[38;5;129;01min\u001b[39;00m token_counts\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    366\u001b[0m   start_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 367\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m subword \u001b[38;5;129;01min\u001b[39;00m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_token_to_subwords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    368\u001b[0m     last_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mlen\u001b[39m(token), start_idx \u001b[38;5;241m+\u001b[39m max_subword_length)\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m end_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, last_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/tensorflow_datasets/core/deprecated/text/subword_text_encoder.py:194\u001b[0m, in \u001b[0;36mSubwordTextEncoder._token_to_subwords\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m start \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(token):\n\u001b[1;32m    192\u001b[0m   subword \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    193\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m end \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\n\u001b[0;32m--> 194\u001b[0m       \u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_subword_len\u001b[49m\u001b[43m)\u001b[49m, start, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    195\u001b[0m   ):\n\u001b[1;32m    196\u001b[0m     candidate \u001b[38;5;241m=\u001b[39m token[start:end]\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    198\u001b[0m         candidate \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_subword_to_id\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m candidate \u001b[38;5;241m==\u001b[39m _UNDERSCORE_REPLACEMENT\n\u001b[1;32m    200\u001b[0m     ):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 한국어 및 영어 데이터 추출\n",
    "korean_texts = df['원문'].astype(str).tolist()\n",
    "english_texts = df['번역문'].astype(str).tolist()\n",
    "\n",
    "# 한국어 텍스트 벡터화\n",
    "vectorizer_ko = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(korean_texts, target_vocab_size=max_token)\n",
    "\n",
    "# 영어 텍스트 벡터화\n",
    "vectorizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(english_texts, target_vocab_size=max_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752ebc49-e028-4c8c-a8ea-f4fabb7a4947",
   "metadata": {},
   "source": [
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ed712f-542a-4e97-9243-f72fda6a2b66",
   "metadata": {},
   "source": [
    "# 오래 걸리는 부분 저장하기\n",
    "with open('vectorizers.pkl', 'wb') as f:\n",
    "    pkl.dump([vectorizer_ko, vectorizer_en], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c19366-b293-4fc9-a28b-f3b945579e7d",
   "metadata": {},
   "source": [
    "# 가져다 쓰기\n",
    "with open('vectorizers.pkl', 'rb') as f:\n",
    "    vectorizer_ko, vectorizer_en = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5f6510-2f1e-40c6-abcd-a054cb15c3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token_ko, end_token_ko = [vectorizer_ko.vocab_size], [vectorizer_ko.vocab_size + 1]\n",
    "start_token_en, end_token_en = [vectorizer_en.vocab_size], [vectorizer_en.vocab_size + 1]\n",
    "vocab_size_ko = vectorizer_ko.vocab_size + 2\n",
    "vocab_size_en = vectorizer_en.vocab_size + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68ac8cb-f032-4fdf-a14d-d389599ae740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시작 토큰 번호 : [19708] [20012]\n",
      "종료 토큰 번호 : [19709] [20013]\n",
      "단어 집합의 크기 : 19710\n",
      "단어 집합의 크기 : 20014\n"
     ]
    }
   ],
   "source": [
    "print('시작 토큰 번호 :',start_token_ko, start_token_en)\n",
    "print('종료 토큰 번호 :',end_token_ko, end_token_en)\n",
    "print('단어 집합의 크기 :',vocab_size_ko)\n",
    "print('단어 집합의 크기 :',vocab_size_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73271cb9-d83b-4f46-900b-527906506987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 길이를 40으로 정의\n",
    "seq_length = seq_len\n",
    "\n",
    "# 토큰화 / 정수 인코딩 / 시작 토큰과 종료 토큰 추가 / 패딩\n",
    "def vectorize_and_filter(src, tar):\n",
    "  vectorized_src, vectorized_tar = [], []\n",
    "  \n",
    "  for (sentence_ko, sentence_en) in zip(src, tar):\n",
    "    # encode(토큰화 + 정수 인코딩), 시작 토큰과 종료 토큰 추가\n",
    "    sentence_ko = start_token_ko + vectorizer_ko.encode(sentence_ko) + end_token_ko\n",
    "    sentence_en = start_token_en + vectorizer_en.encode(sentence_en) + end_token_en\n",
    "      \n",
    "    if len(sentence_ko) <= seq_length and len(sentence_en) <= seq_length:\n",
    "      vectorized_src.append(sentence_ko)\n",
    "      vectorized_tar.append(sentence_en)\n",
    "  \n",
    "  # 패딩\n",
    "  vectorized_src = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      vectorized_src, maxlen=seq_length, padding='post')\n",
    "  vectorized_tar = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      vectorized_tar, maxlen=seq_length, padding='post')\n",
    "  \n",
    "  return vectorized_src, vectorized_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962ec049-19d8-4879-b7ff-598cbe947da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_text, en_text = vectorize_and_filter(korean_texts, english_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecd4a9c-c3cc-44db-af10-15703811ee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, validation, test set 나누기\n",
    "korean_train, korean_val, korean_test = ko_text[:180000], ko_text[180000:195000], ko_text[195000:]\n",
    "english_train, english_val, english_test = en_text[:180000], en_text[180000:195000], en_text[195000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c068684a-1169-43d0-8f7d-bf6d6ca3b9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "buffer_size = len(en_text)\n",
    "\n",
    "dataset_train = tf.data.Dataset.from_tensor_slices((korean_train, english_train)).batch(batch_size).shuffle(buffer_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "dataset_val = tf.data.Dataset.from_tensor_slices((korean_val, english_val)).batch(batch_size).shuffle(buffer_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "dataset_test = tf.data.Dataset.from_tensor_slices((korean_test, english_test)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bbe95f-8d23-416e-b14b-521076634eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(dim_model=dim_model,num_head=num_head,dim_ff=dim_ff,num_layer=num_layer,input_vocab_size=vocab_size_ko,output_vocab_size=vocab_size_en,seq_len=seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ccbefb-4811-4192-8f8b-7edbd636a794",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(dim_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  # ensure labels have shape (batch_size, MAX_LENGTH - 1)\n",
    "  y_true = tf.reshape(y_true, shape=(-1, seq_len - 1))\n",
    "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "transformer.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469763ca-9863-46d4-97d7-b9293aa77ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인풋, 아웃풋의 텐셔 shape 정의\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n",
    "]\n",
    "\n",
    "# tf.function을 사용하면 그래프를 미리 컴파일 하기 때문에 속도가 상당히 빠름\n",
    "# 같은 GPU여도 케라스에 비해서 체감상 7~8배 정도의 차이가 나는 것 같음\n",
    "# @tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "  tar_inp = tar[:, :-1]\n",
    "  tar_real = tar[:, 1:]\n",
    "  \n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = transformer(inp, tar_inp)\n",
    "    loss = loss_function(tar_real, predictions)\n",
    "\n",
    "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "  \n",
    "  train_loss(loss)\n",
    "  train_accuracy(tar_real, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9865d96a-89d9-4667-8b42-beb26cd37d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장할 체크포인트 지정\n",
    "checkpoint_path = \"./\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88a16a4-2fad-4d76-b821-9105fd451987",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 9.7733 Accuracy 0.0000\n",
      "Epoch 1 Batch 50 Loss 9.6976 Accuracy 0.0001\n",
      "Epoch 1 Batch 100 Loss 9.4825 Accuracy 0.2449\n",
      "Epoch 1 Batch 150 Loss 9.1693 Accuracy 0.3824\n",
      "Epoch 1 Batch 200 Loss 8.8132 Accuracy 0.4560\n",
      "Epoch 1 Batch 250 Loss 8.4292 Accuracy 0.5016\n",
      "Epoch 1 Batch 300 Loss 8.0028 Accuracy 0.5338\n",
      "Epoch 1 Batch 350 Loss 7.5450 Accuracy 0.5552\n",
      "Epoch 1 Batch 400 Loss 7.0635 Accuracy 0.5712\n",
      "Epoch 1 Batch 450 Loss 6.6015 Accuracy 0.5834\n",
      "Epoch 1 Batch 500 Loss 6.1958 Accuracy 0.5936\n",
      "Epoch 1 Batch 550 Loss 5.8547 Accuracy 0.6013\n",
      "Epoch 1 Batch 600 Loss 5.5478 Accuracy 0.6106\n",
      "Epoch 1 Batch 650 Loss 5.2904 Accuracy 0.6182\n",
      "Epoch 1 Batch 700 Loss 5.0557 Accuracy 0.6262\n",
      "Epoch 1 Batch 750 Loss 4.8640 Accuracy 0.6319\n",
      "Epoch 1 Batch 800 Loss 4.6850 Accuracy 0.6379\n",
      "Epoch 1 Batch 850 Loss 4.5273 Accuracy 0.6429\n",
      "Epoch 1 Batch 900 Loss 4.3858 Accuracy 0.6477\n",
      "Epoch 1 Batch 950 Loss 4.2623 Accuracy 0.6514\n",
      "Epoch 1 Batch 1000 Loss 4.1444 Accuracy 0.6556\n",
      "Epoch 1 Batch 1050 Loss 4.0409 Accuracy 0.6591\n",
      "Epoch 1 Batch 1100 Loss 3.9440 Accuracy 0.6626\n",
      "Epoch 1 Batch 1150 Loss 3.8582 Accuracy 0.6656\n",
      "Epoch 1 Batch 1200 Loss 3.7769 Accuracy 0.6685\n",
      "Epoch 1 Batch 1250 Loss 3.6988 Accuracy 0.6716\n",
      "Epoch 1 Batch 1300 Loss 3.6275 Accuracy 0.6743\n",
      "Epoch 1 Batch 1350 Loss 3.5583 Accuracy 0.6772\n",
      "Epoch 1 Batch 1400 Loss 3.5000 Accuracy 0.6791\n",
      "Epoch 1 Batch 1450 Loss 3.4417 Accuracy 0.6814\n",
      "Epoch 1 Batch 1500 Loss 3.3905 Accuracy 0.6833\n",
      "Epoch 1 Batch 1550 Loss 3.3390 Accuracy 0.6854\n",
      "Epoch 1 Batch 1600 Loss 3.2884 Accuracy 0.6876\n",
      "Epoch 1 Batch 1650 Loss 3.2416 Accuracy 0.6896\n",
      "Epoch 1 Batch 1700 Loss 3.1983 Accuracy 0.6913\n",
      "Epoch 1 Batch 1750 Loss 3.1559 Accuracy 0.6932\n",
      "Epoch 1 Batch 1800 Loss 3.1192 Accuracy 0.6946\n",
      "Epoch 1 Batch 1850 Loss 3.0822 Accuracy 0.6961\n",
      "Epoch 1 Batch 1900 Loss 3.0464 Accuracy 0.6975\n",
      "Epoch 1 Batch 1950 Loss 3.0120 Accuracy 0.6990\n",
      "Epoch 1 Batch 2000 Loss 2.9819 Accuracy 0.7001\n",
      "Epoch 1 Batch 2050 Loss 2.9487 Accuracy 0.7017\n",
      "Epoch 1 Batch 2100 Loss 2.9202 Accuracy 0.7028\n",
      "Epoch 1 Batch 2150 Loss 2.8929 Accuracy 0.7040\n",
      "Epoch 1 Batch 2200 Loss 2.8650 Accuracy 0.7052\n",
      "Epoch 1 Batch 2250 Loss 2.8407 Accuracy 0.7062\n",
      "Epoch 1 Batch 2300 Loss 2.8178 Accuracy 0.7070\n",
      "Epoch 1 Batch 2350 Loss 2.7936 Accuracy 0.7082\n",
      "Epoch 1 Batch 2400 Loss 2.7706 Accuracy 0.7092\n",
      "Epoch 1 Batch 2450 Loss 2.7455 Accuracy 0.7105\n",
      "Epoch 1 Batch 2500 Loss 2.7235 Accuracy 0.7115\n",
      "Epoch 1 Batch 2550 Loss 2.7013 Accuracy 0.7126\n",
      "Epoch 1 Batch 2600 Loss 2.6824 Accuracy 0.7134\n",
      "Epoch 1 Batch 2650 Loss 2.6635 Accuracy 0.7143\n",
      "Epoch 1 Batch 2700 Loss 2.6456 Accuracy 0.7150\n",
      "Epoch 1 Batch 2750 Loss 2.6270 Accuracy 0.7159\n",
      "Epoch 1 Batch 2800 Loss 2.6079 Accuracy 0.7169\n",
      "Epoch 1 Batch 2850 Loss 2.5904 Accuracy 0.7178\n",
      "Epoch 1 Batch 2900 Loss 2.5723 Accuracy 0.7187\n",
      "Epoch 1 Batch 2950 Loss 2.5565 Accuracy 0.7194\n",
      "Epoch 1 Batch 3000 Loss 2.5397 Accuracy 0.7203\n",
      "Epoch 1 Batch 3050 Loss 2.5267 Accuracy 0.7208\n",
      "Epoch 1 Batch 3100 Loss 2.5131 Accuracy 0.7214\n",
      "Epoch 1 Batch 3150 Loss 2.4997 Accuracy 0.7220\n",
      "Epoch 1 Batch 3200 Loss 2.4856 Accuracy 0.7227\n",
      "Epoch 1 Batch 3250 Loss 2.4739 Accuracy 0.7232\n",
      "Epoch 1 Batch 3300 Loss 2.4609 Accuracy 0.7239\n",
      "Epoch 1 Batch 3350 Loss 2.4482 Accuracy 0.7245\n",
      "Epoch 1 Batch 3400 Loss 2.4347 Accuracy 0.7252\n",
      "Epoch 1 Batch 3450 Loss 2.4225 Accuracy 0.7258\n",
      "Epoch 1 Batch 3500 Loss 2.4119 Accuracy 0.7262\n",
      "Epoch 1 Batch 3550 Loss 2.4007 Accuracy 0.7268\n",
      "Epoch 1 Batch 3600 Loss 2.3893 Accuracy 0.7274\n",
      "Epoch 1 Batch 3650 Loss 2.3768 Accuracy 0.7281\n",
      "Epoch 1 Batch 3700 Loss 2.3660 Accuracy 0.7286\n",
      "Epoch 1 Batch 3750 Loss 2.3552 Accuracy 0.7292\n",
      "Epoch 1 Batch 3800 Loss 2.3459 Accuracy 0.7296\n",
      "Epoch 1 Batch 3850 Loss 2.3353 Accuracy 0.7302\n",
      "Epoch 1 Batch 3900 Loss 2.3247 Accuracy 0.7308\n",
      "Epoch 1 Batch 3950 Loss 2.3137 Accuracy 0.7314\n",
      "Epoch 1 Batch 4000 Loss 2.3036 Accuracy 0.7320\n",
      "Epoch 1 Batch 4050 Loss 2.2945 Accuracy 0.7324\n",
      "Epoch 1 Batch 4100 Loss 2.2850 Accuracy 0.7329\n",
      "Epoch 1 Batch 4150 Loss 2.2755 Accuracy 0.7335\n",
      "Epoch 1 Batch 4200 Loss 2.2678 Accuracy 0.7338\n",
      "Epoch 1 Batch 4250 Loss 2.2587 Accuracy 0.7343\n",
      "Epoch 1 Batch 4300 Loss 2.2497 Accuracy 0.7348\n",
      "Epoch 1 Batch 4350 Loss 2.2397 Accuracy 0.7354\n",
      "Epoch 1 Batch 4400 Loss 2.2324 Accuracy 0.7358\n",
      "Epoch 1 Batch 4450 Loss 2.2243 Accuracy 0.7362\n",
      "Epoch 1 Batch 4500 Loss 2.2169 Accuracy 0.7366\n",
      "Epoch 1 Batch 4550 Loss 2.2106 Accuracy 0.7369\n",
      "Epoch 1 Batch 4600 Loss 2.2029 Accuracy 0.7374\n",
      "Epoch 1 Batch 4650 Loss 2.1955 Accuracy 0.7378\n",
      "Epoch 1 Batch 4700 Loss 2.1883 Accuracy 0.7381\n",
      "Epoch 1 Batch 4750 Loss 2.1813 Accuracy 0.7385\n",
      "Epoch 1 Batch 4800 Loss 2.1740 Accuracy 0.7389\n",
      "Epoch 1 Batch 4850 Loss 2.1669 Accuracy 0.7393\n",
      "Epoch 1 Batch 4900 Loss 2.1595 Accuracy 0.7397\n",
      "Epoch 1 Batch 4950 Loss 2.1529 Accuracy 0.7401\n",
      "Epoch 1 Batch 5000 Loss 2.1460 Accuracy 0.7405\n",
      "Epoch 1 Batch 5050 Loss 2.1408 Accuracy 0.7407\n",
      "Epoch 1 Batch 5100 Loss 2.1331 Accuracy 0.7412\n",
      "Epoch 1 Batch 5150 Loss 2.1258 Accuracy 0.7416\n",
      "Epoch 1 Batch 5200 Loss 2.1189 Accuracy 0.7421\n",
      "Epoch 1 Batch 5250 Loss 2.1133 Accuracy 0.7424\n",
      "Epoch 1 Batch 5300 Loss 2.1071 Accuracy 0.7427\n",
      "Epoch 1 Batch 5350 Loss 2.1020 Accuracy 0.7430\n",
      "Epoch 1 Batch 5400 Loss 2.0961 Accuracy 0.7433\n",
      "Epoch 1 Batch 5450 Loss 2.0896 Accuracy 0.7437\n",
      "Epoch 1 Batch 5500 Loss 2.0840 Accuracy 0.7440\n",
      "Epoch 1 Batch 5550 Loss 2.0792 Accuracy 0.7443\n",
      "Epoch 1 Batch 5600 Loss 2.0738 Accuracy 0.7446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 00:51:26.426215: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 2.0705 Accuracy 0.7448\n",
      "Time taken for 1 epoch: 6483.745259046555 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.8574 Accuracy 0.8606\n",
      "Epoch 2 Batch 50 Loss 1.3828 Accuracy 0.7859\n",
      "Epoch 2 Batch 100 Loss 1.4151 Accuracy 0.7818\n",
      "Epoch 2 Batch 150 Loss 1.3997 Accuracy 0.7842\n",
      "Epoch 2 Batch 200 Loss 1.3747 Accuracy 0.7875\n",
      "Epoch 2 Batch 250 Loss 1.3894 Accuracy 0.7861\n",
      "Epoch 2 Batch 300 Loss 1.3875 Accuracy 0.7854\n",
      "Epoch 2 Batch 350 Loss 1.3830 Accuracy 0.7863\n",
      "Epoch 2 Batch 400 Loss 1.3807 Accuracy 0.7869\n",
      "Epoch 2 Batch 450 Loss 1.3855 Accuracy 0.7863\n",
      "Epoch 2 Batch 500 Loss 1.3780 Accuracy 0.7871\n",
      "Epoch 2 Batch 550 Loss 1.3696 Accuracy 0.7882\n",
      "Epoch 2 Batch 600 Loss 1.3784 Accuracy 0.7874\n",
      "Epoch 2 Batch 650 Loss 1.3765 Accuracy 0.7875\n",
      "Epoch 2 Batch 700 Loss 1.3738 Accuracy 0.7876\n",
      "Epoch 2 Batch 750 Loss 1.3711 Accuracy 0.7880\n",
      "Epoch 2 Batch 800 Loss 1.3734 Accuracy 0.7879\n",
      "Epoch 2 Batch 850 Loss 1.3641 Accuracy 0.7891\n",
      "Epoch 2 Batch 900 Loss 1.3672 Accuracy 0.7889\n",
      "Epoch 2 Batch 950 Loss 1.3739 Accuracy 0.7881\n",
      "Epoch 2 Batch 1000 Loss 1.3790 Accuracy 0.7875\n",
      "Epoch 2 Batch 1050 Loss 1.3765 Accuracy 0.7877\n",
      "Epoch 2 Batch 1100 Loss 1.3783 Accuracy 0.7876\n",
      "Epoch 2 Batch 1150 Loss 1.3802 Accuracy 0.7874\n",
      "Epoch 2 Batch 1200 Loss 1.3768 Accuracy 0.7879\n",
      "Epoch 2 Batch 1250 Loss 1.3739 Accuracy 0.7882\n",
      "Epoch 2 Batch 1300 Loss 1.3703 Accuracy 0.7886\n",
      "Epoch 2 Batch 1350 Loss 1.3692 Accuracy 0.7887\n",
      "Epoch 2 Batch 1400 Loss 1.3687 Accuracy 0.7888\n",
      "Epoch 2 Batch 1450 Loss 1.3680 Accuracy 0.7888\n",
      "Epoch 2 Batch 1500 Loss 1.3722 Accuracy 0.7884\n",
      "Epoch 2 Batch 1550 Loss 1.3678 Accuracy 0.7889\n",
      "Epoch 2 Batch 1600 Loss 1.3679 Accuracy 0.7889\n",
      "Epoch 2 Batch 1650 Loss 1.3665 Accuracy 0.7891\n",
      "Epoch 2 Batch 1700 Loss 1.3673 Accuracy 0.7889\n",
      "Epoch 2 Batch 1750 Loss 1.3676 Accuracy 0.7889\n",
      "Epoch 2 Batch 1800 Loss 1.3703 Accuracy 0.7885\n",
      "Epoch 2 Batch 1850 Loss 1.3696 Accuracy 0.7887\n",
      "Epoch 2 Batch 1900 Loss 1.3684 Accuracy 0.7888\n",
      "Epoch 2 Batch 1950 Loss 1.3659 Accuracy 0.7891\n",
      "Epoch 2 Batch 2000 Loss 1.3665 Accuracy 0.7890\n",
      "Epoch 2 Batch 2050 Loss 1.3655 Accuracy 0.7892\n",
      "Epoch 2 Batch 2100 Loss 1.3662 Accuracy 0.7890\n",
      "Epoch 2 Batch 2150 Loss 1.3647 Accuracy 0.7892\n",
      "Epoch 2 Batch 2200 Loss 1.3671 Accuracy 0.7889\n",
      "Epoch 2 Batch 2250 Loss 1.3670 Accuracy 0.7890\n",
      "Epoch 2 Batch 2300 Loss 1.3671 Accuracy 0.7889\n",
      "Epoch 2 Batch 2350 Loss 1.3678 Accuracy 0.7887\n",
      "Epoch 2 Batch 2400 Loss 1.3680 Accuracy 0.7887\n",
      "Epoch 2 Batch 2450 Loss 1.3677 Accuracy 0.7888\n",
      "Epoch 2 Batch 2500 Loss 1.3681 Accuracy 0.7886\n",
      "Epoch 2 Batch 2550 Loss 1.3692 Accuracy 0.7885\n",
      "Epoch 2 Batch 2600 Loss 1.3686 Accuracy 0.7885\n",
      "Epoch 2 Batch 2650 Loss 1.3691 Accuracy 0.7883\n",
      "Epoch 2 Batch 2700 Loss 1.3687 Accuracy 0.7883\n",
      "Epoch 2 Batch 2750 Loss 1.3683 Accuracy 0.7884\n",
      "Epoch 2 Batch 2800 Loss 1.3672 Accuracy 0.7885\n",
      "Epoch 2 Batch 2850 Loss 1.3673 Accuracy 0.7885\n",
      "Epoch 2 Batch 2900 Loss 1.3673 Accuracy 0.7885\n",
      "Epoch 2 Batch 2950 Loss 1.3670 Accuracy 0.7885\n",
      "Epoch 2 Batch 3000 Loss 1.3668 Accuracy 0.7885\n",
      "Epoch 2 Batch 3050 Loss 1.3650 Accuracy 0.7887\n",
      "Epoch 2 Batch 3100 Loss 1.3659 Accuracy 0.7886\n",
      "Epoch 2 Batch 3150 Loss 1.3651 Accuracy 0.7887\n",
      "Epoch 2 Batch 3200 Loss 1.3633 Accuracy 0.7889\n",
      "Epoch 2 Batch 3250 Loss 1.3628 Accuracy 0.7890\n",
      "Epoch 2 Batch 3300 Loss 1.3631 Accuracy 0.7890\n",
      "Epoch 2 Batch 3350 Loss 1.3638 Accuracy 0.7888\n",
      "Epoch 2 Batch 3400 Loss 1.3634 Accuracy 0.7889\n",
      "Epoch 2 Batch 3450 Loss 1.3620 Accuracy 0.7890\n",
      "Epoch 2 Batch 3500 Loss 1.3617 Accuracy 0.7890\n",
      "Epoch 2 Batch 3550 Loss 1.3611 Accuracy 0.7891\n",
      "Epoch 2 Batch 3600 Loss 1.3606 Accuracy 0.7891\n",
      "Epoch 2 Batch 3650 Loss 1.3614 Accuracy 0.7890\n",
      "Epoch 2 Batch 3700 Loss 1.3607 Accuracy 0.7890\n",
      "Epoch 2 Batch 3750 Loss 1.3604 Accuracy 0.7891\n",
      "Epoch 2 Batch 3800 Loss 1.3615 Accuracy 0.7889\n",
      "Epoch 2 Batch 3850 Loss 1.3609 Accuracy 0.7890\n",
      "Epoch 2 Batch 3900 Loss 1.3612 Accuracy 0.7890\n",
      "Epoch 2 Batch 3950 Loss 1.3614 Accuracy 0.7889\n",
      "Epoch 2 Batch 4000 Loss 1.3600 Accuracy 0.7891\n",
      "Epoch 2 Batch 4050 Loss 1.3591 Accuracy 0.7893\n",
      "Epoch 2 Batch 4100 Loss 1.3590 Accuracy 0.7893\n",
      "Epoch 2 Batch 4150 Loss 1.3583 Accuracy 0.7893\n",
      "Epoch 2 Batch 4200 Loss 1.3575 Accuracy 0.7894\n",
      "Epoch 2 Batch 4250 Loss 1.3573 Accuracy 0.7894\n",
      "Epoch 2 Batch 4300 Loss 1.3580 Accuracy 0.7893\n",
      "Epoch 2 Batch 4350 Loss 1.3592 Accuracy 0.7892\n",
      "Epoch 2 Batch 4400 Loss 1.3584 Accuracy 0.7894\n",
      "Epoch 2 Batch 4450 Loss 1.3575 Accuracy 0.7895\n",
      "Epoch 2 Batch 4500 Loss 1.3568 Accuracy 0.7895\n",
      "Epoch 2 Batch 4550 Loss 1.3564 Accuracy 0.7896\n",
      "Epoch 2 Batch 4600 Loss 1.3570 Accuracy 0.7895\n",
      "Epoch 2 Batch 4650 Loss 1.3574 Accuracy 0.7895\n",
      "Epoch 2 Batch 4700 Loss 1.3576 Accuracy 0.7894\n",
      "Epoch 2 Batch 4750 Loss 1.3565 Accuracy 0.7895\n",
      "Epoch 2 Batch 4800 Loss 1.3564 Accuracy 0.7895\n",
      "Epoch 2 Batch 4850 Loss 1.3561 Accuracy 0.7895\n",
      "Epoch 2 Batch 4900 Loss 1.3553 Accuracy 0.7896\n",
      "Epoch 2 Batch 4950 Loss 1.3554 Accuracy 0.7896\n",
      "Epoch 2 Batch 5000 Loss 1.3552 Accuracy 0.7896\n",
      "Epoch 2 Batch 5050 Loss 1.3548 Accuracy 0.7897\n",
      "Epoch 2 Batch 5100 Loss 1.3542 Accuracy 0.7897\n",
      "Epoch 2 Batch 5150 Loss 1.3549 Accuracy 0.7897\n",
      "Epoch 2 Batch 5200 Loss 1.3555 Accuracy 0.7896\n",
      "Epoch 2 Batch 5250 Loss 1.3558 Accuracy 0.7896\n",
      "Epoch 2 Batch 5300 Loss 1.3560 Accuracy 0.7895\n",
      "Epoch 2 Batch 5350 Loss 1.3563 Accuracy 0.7895\n",
      "Epoch 2 Batch 5400 Loss 1.3561 Accuracy 0.7896\n",
      "Epoch 2 Batch 5450 Loss 1.3570 Accuracy 0.7895\n",
      "Epoch 2 Batch 5500 Loss 1.3569 Accuracy 0.7895\n",
      "Epoch 2 Batch 5550 Loss 1.3574 Accuracy 0.7894\n",
      "Epoch 2 Batch 5600 Loss 1.3576 Accuracy 0.7894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 02:39:54.611024: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss 1.3572 Accuracy 0.7894\n",
      "Time taken for 1 epoch: 6508.186465978622 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.6318 Accuracy 0.9079\n",
      "Epoch 3 Batch 50 Loss 1.2073 Accuracy 0.8103\n",
      "Epoch 3 Batch 100 Loss 1.2505 Accuracy 0.8035\n",
      "Epoch 3 Batch 150 Loss 1.2226 Accuracy 0.8065\n",
      "Epoch 3 Batch 200 Loss 1.2227 Accuracy 0.8067\n",
      "Epoch 3 Batch 250 Loss 1.2028 Accuracy 0.8086\n",
      "Epoch 3 Batch 300 Loss 1.1863 Accuracy 0.8114\n",
      "Epoch 3 Batch 350 Loss 1.2066 Accuracy 0.8082\n",
      "Epoch 3 Batch 400 Loss 1.2010 Accuracy 0.8087\n",
      "Epoch 3 Batch 450 Loss 1.2087 Accuracy 0.8079\n",
      "Epoch 3 Batch 500 Loss 1.2175 Accuracy 0.8065\n",
      "Epoch 3 Batch 550 Loss 1.2276 Accuracy 0.8053\n",
      "Epoch 3 Batch 600 Loss 1.2269 Accuracy 0.8053\n",
      "Epoch 3 Batch 650 Loss 1.2328 Accuracy 0.8045\n",
      "Epoch 3 Batch 700 Loss 1.2326 Accuracy 0.8044\n",
      "Epoch 3 Batch 750 Loss 1.2361 Accuracy 0.8040\n",
      "Epoch 3 Batch 800 Loss 1.2349 Accuracy 0.8041\n",
      "Epoch 3 Batch 850 Loss 1.2378 Accuracy 0.8035\n",
      "Epoch 3 Batch 900 Loss 1.2372 Accuracy 0.8037\n",
      "Epoch 3 Batch 950 Loss 1.2337 Accuracy 0.8040\n",
      "Epoch 3 Batch 1000 Loss 1.2309 Accuracy 0.8045\n",
      "Epoch 3 Batch 1050 Loss 1.2314 Accuracy 0.8044\n",
      "Epoch 3 Batch 1100 Loss 1.2263 Accuracy 0.8052\n",
      "Epoch 3 Batch 1150 Loss 1.2260 Accuracy 0.8054\n",
      "Epoch 3 Batch 1200 Loss 1.2240 Accuracy 0.8057\n",
      "Epoch 3 Batch 1250 Loss 1.2260 Accuracy 0.8055\n",
      "Epoch 3 Batch 1300 Loss 1.2266 Accuracy 0.8055\n",
      "Epoch 3 Batch 1350 Loss 1.2286 Accuracy 0.8052\n",
      "Epoch 3 Batch 1400 Loss 1.2327 Accuracy 0.8046\n",
      "Epoch 3 Batch 1450 Loss 1.2317 Accuracy 0.8048\n",
      "Epoch 3 Batch 1500 Loss 1.2343 Accuracy 0.8045\n",
      "Epoch 3 Batch 1550 Loss 1.2337 Accuracy 0.8046\n",
      "Epoch 3 Batch 1600 Loss 1.2329 Accuracy 0.8047\n",
      "Epoch 3 Batch 1650 Loss 1.2332 Accuracy 0.8047\n",
      "Epoch 3 Batch 1700 Loss 1.2347 Accuracy 0.8046\n",
      "Epoch 3 Batch 1750 Loss 1.2345 Accuracy 0.8046\n",
      "Epoch 3 Batch 1800 Loss 1.2356 Accuracy 0.8044\n",
      "Epoch 3 Batch 1850 Loss 1.2353 Accuracy 0.8045\n",
      "Epoch 3 Batch 1900 Loss 1.2359 Accuracy 0.8045\n",
      "Epoch 3 Batch 1950 Loss 1.2366 Accuracy 0.8044\n",
      "Epoch 3 Batch 2000 Loss 1.2382 Accuracy 0.8042\n",
      "Epoch 3 Batch 2050 Loss 1.2381 Accuracy 0.8042\n",
      "Epoch 3 Batch 2100 Loss 1.2363 Accuracy 0.8044\n",
      "Epoch 3 Batch 2150 Loss 1.2354 Accuracy 0.8045\n",
      "Epoch 3 Batch 2200 Loss 1.2356 Accuracy 0.8045\n",
      "Epoch 3 Batch 2250 Loss 1.2363 Accuracy 0.8044\n",
      "Epoch 3 Batch 2300 Loss 1.2377 Accuracy 0.8042\n",
      "Epoch 3 Batch 2350 Loss 1.2369 Accuracy 0.8044\n",
      "Epoch 3 Batch 2400 Loss 1.2361 Accuracy 0.8045\n",
      "Epoch 3 Batch 2450 Loss 1.2386 Accuracy 0.8042\n",
      "Epoch 3 Batch 2500 Loss 1.2369 Accuracy 0.8044\n",
      "Epoch 3 Batch 2550 Loss 1.2381 Accuracy 0.8043\n",
      "Epoch 3 Batch 2600 Loss 1.2391 Accuracy 0.8042\n",
      "Epoch 3 Batch 2650 Loss 1.2406 Accuracy 0.8040\n",
      "Epoch 3 Batch 2700 Loss 1.2396 Accuracy 0.8042\n",
      "Epoch 3 Batch 2750 Loss 1.2399 Accuracy 0.8041\n",
      "Epoch 3 Batch 2800 Loss 1.2415 Accuracy 0.8039\n",
      "Epoch 3 Batch 2850 Loss 1.2427 Accuracy 0.8038\n",
      "Epoch 3 Batch 2900 Loss 1.2432 Accuracy 0.8037\n",
      "Epoch 3 Batch 2950 Loss 1.2420 Accuracy 0.8039\n",
      "Epoch 3 Batch 3000 Loss 1.2420 Accuracy 0.8039\n",
      "Epoch 3 Batch 3050 Loss 1.2423 Accuracy 0.8038\n",
      "Epoch 3 Batch 3100 Loss 1.2454 Accuracy 0.8035\n",
      "Epoch 3 Batch 3150 Loss 1.2468 Accuracy 0.8034\n",
      "Epoch 3 Batch 3200 Loss 1.2471 Accuracy 0.8034\n",
      "Epoch 3 Batch 3250 Loss 1.2471 Accuracy 0.8034\n",
      "Epoch 3 Batch 3300 Loss 1.2483 Accuracy 0.8032\n",
      "Epoch 3 Batch 3350 Loss 1.2477 Accuracy 0.8033\n",
      "Epoch 3 Batch 3400 Loss 1.2477 Accuracy 0.8034\n",
      "Epoch 3 Batch 3450 Loss 1.2482 Accuracy 0.8033\n",
      "Epoch 3 Batch 3500 Loss 1.2482 Accuracy 0.8033\n",
      "Epoch 3 Batch 3550 Loss 1.2474 Accuracy 0.8035\n",
      "Epoch 3 Batch 3600 Loss 1.2462 Accuracy 0.8037\n",
      "Epoch 3 Batch 3650 Loss 1.2472 Accuracy 0.8036\n",
      "Epoch 3 Batch 3700 Loss 1.2470 Accuracy 0.8036\n",
      "Epoch 3 Batch 3750 Loss 1.2471 Accuracy 0.8036\n",
      "Epoch 3 Batch 3800 Loss 1.2479 Accuracy 0.8035\n",
      "Epoch 3 Batch 3850 Loss 1.2480 Accuracy 0.8035\n",
      "Epoch 3 Batch 3900 Loss 1.2499 Accuracy 0.8033\n",
      "Epoch 3 Batch 3950 Loss 1.2514 Accuracy 0.8032\n",
      "Epoch 3 Batch 4000 Loss 1.2503 Accuracy 0.8033\n",
      "Epoch 3 Batch 4050 Loss 1.2515 Accuracy 0.8032\n",
      "Epoch 3 Batch 4100 Loss 1.2504 Accuracy 0.8033\n",
      "Epoch 3 Batch 4150 Loss 1.2507 Accuracy 0.8033\n",
      "Epoch 3 Batch 4200 Loss 1.2503 Accuracy 0.8034\n",
      "Epoch 3 Batch 4250 Loss 1.2503 Accuracy 0.8034\n",
      "Epoch 3 Batch 4300 Loss 1.2491 Accuracy 0.8035\n",
      "Epoch 3 Batch 4350 Loss 1.2491 Accuracy 0.8035\n",
      "Epoch 3 Batch 4400 Loss 1.2495 Accuracy 0.8035\n",
      "Epoch 3 Batch 4450 Loss 1.2490 Accuracy 0.8036\n",
      "Epoch 3 Batch 4500 Loss 1.2497 Accuracy 0.8035\n",
      "Epoch 3 Batch 4550 Loss 1.2511 Accuracy 0.8034\n",
      "Epoch 3 Batch 4600 Loss 1.2515 Accuracy 0.8033\n",
      "Epoch 3 Batch 4650 Loss 1.2514 Accuracy 0.8033\n",
      "Epoch 3 Batch 4700 Loss 1.2510 Accuracy 0.8034\n",
      "Epoch 3 Batch 4750 Loss 1.2511 Accuracy 0.8034\n",
      "Epoch 3 Batch 4800 Loss 1.2506 Accuracy 0.8035\n",
      "Epoch 3 Batch 4850 Loss 1.2509 Accuracy 0.8035\n",
      "Epoch 3 Batch 4900 Loss 1.2512 Accuracy 0.8034\n",
      "Epoch 3 Batch 4950 Loss 1.2508 Accuracy 0.8035\n",
      "Epoch 3 Batch 5000 Loss 1.2506 Accuracy 0.8035\n",
      "Epoch 3 Batch 5050 Loss 1.2504 Accuracy 0.8036\n",
      "Epoch 3 Batch 5100 Loss 1.2503 Accuracy 0.8036\n",
      "Epoch 3 Batch 5150 Loss 1.2499 Accuracy 0.8037\n",
      "Epoch 3 Batch 5200 Loss 1.2492 Accuracy 0.8038\n",
      "Epoch 3 Batch 5250 Loss 1.2499 Accuracy 0.8037\n",
      "Epoch 3 Batch 5300 Loss 1.2499 Accuracy 0.8037\n",
      "Epoch 3 Batch 5350 Loss 1.2507 Accuracy 0.8036\n",
      "Epoch 3 Batch 5400 Loss 1.2511 Accuracy 0.8036\n",
      "Epoch 3 Batch 5450 Loss 1.2510 Accuracy 0.8037\n",
      "Epoch 3 Batch 5500 Loss 1.2501 Accuracy 0.8038\n",
      "Epoch 3 Batch 5550 Loss 1.2510 Accuracy 0.8037\n",
      "Epoch 3 Batch 5600 Loss 1.2504 Accuracy 0.8038\n",
      "Epoch 3 Loss 1.2507 Accuracy 0.8038\n",
      "Time taken for 1 epoch: 6491.347101926804 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.4278 Accuracy 0.9183\n",
      "Epoch 4 Batch 50 Loss 1.1058 Accuracy 0.8249\n",
      "Epoch 4 Batch 100 Loss 1.1676 Accuracy 0.8178\n",
      "Epoch 4 Batch 150 Loss 1.1435 Accuracy 0.8209\n",
      "Epoch 4 Batch 200 Loss 1.1198 Accuracy 0.8234\n",
      "Epoch 4 Batch 250 Loss 1.1295 Accuracy 0.8216\n",
      "Epoch 4 Batch 300 Loss 1.1357 Accuracy 0.8208\n",
      "Epoch 4 Batch 350 Loss 1.1442 Accuracy 0.8202\n",
      "Epoch 4 Batch 400 Loss 1.1654 Accuracy 0.8179\n",
      "Epoch 4 Batch 450 Loss 1.1705 Accuracy 0.8171\n",
      "Epoch 4 Batch 500 Loss 1.1733 Accuracy 0.8169\n",
      "Epoch 4 Batch 550 Loss 1.1641 Accuracy 0.8179\n",
      "Epoch 4 Batch 600 Loss 1.1726 Accuracy 0.8172\n",
      "Epoch 4 Batch 650 Loss 1.1637 Accuracy 0.8184\n",
      "Epoch 4 Batch 700 Loss 1.1699 Accuracy 0.8173\n",
      "Epoch 4 Batch 750 Loss 1.1684 Accuracy 0.8175\n",
      "Epoch 4 Batch 800 Loss 1.1714 Accuracy 0.8172\n",
      "Epoch 4 Batch 850 Loss 1.1710 Accuracy 0.8170\n",
      "Epoch 4 Batch 900 Loss 1.1704 Accuracy 0.8172\n",
      "Epoch 4 Batch 950 Loss 1.1686 Accuracy 0.8174\n",
      "Epoch 4 Batch 1000 Loss 1.1697 Accuracy 0.8173\n",
      "Epoch 4 Batch 1050 Loss 1.1692 Accuracy 0.8173\n",
      "Epoch 4 Batch 1100 Loss 1.1658 Accuracy 0.8177\n",
      "Epoch 4 Batch 1150 Loss 1.1674 Accuracy 0.8175\n",
      "Epoch 4 Batch 1200 Loss 1.1643 Accuracy 0.8178\n",
      "Epoch 4 Batch 1250 Loss 1.1662 Accuracy 0.8176\n",
      "Epoch 4 Batch 1300 Loss 1.1656 Accuracy 0.8176\n",
      "Epoch 4 Batch 1350 Loss 1.1721 Accuracy 0.8167\n",
      "Epoch 4 Batch 1400 Loss 1.1743 Accuracy 0.8165\n",
      "Epoch 4 Batch 1450 Loss 1.1705 Accuracy 0.8169\n",
      "Epoch 4 Batch 1500 Loss 1.1714 Accuracy 0.8169\n",
      "Epoch 4 Batch 1550 Loss 1.1742 Accuracy 0.8166\n",
      "Epoch 4 Batch 1600 Loss 1.1737 Accuracy 0.8167\n",
      "Epoch 4 Batch 1650 Loss 1.1767 Accuracy 0.8163\n",
      "Epoch 4 Batch 1700 Loss 1.1761 Accuracy 0.8164\n",
      "Epoch 4 Batch 1750 Loss 1.1732 Accuracy 0.8166\n",
      "Epoch 4 Batch 1800 Loss 1.1721 Accuracy 0.8167\n",
      "Epoch 4 Batch 1850 Loss 1.1739 Accuracy 0.8165\n",
      "Epoch 4 Batch 1900 Loss 1.1744 Accuracy 0.8164\n",
      "Epoch 4 Batch 1950 Loss 1.1779 Accuracy 0.8160\n",
      "Epoch 4 Batch 2000 Loss 1.1790 Accuracy 0.8158\n",
      "Epoch 4 Batch 2050 Loss 1.1798 Accuracy 0.8157\n",
      "Epoch 4 Batch 2100 Loss 1.1781 Accuracy 0.8159\n",
      "Epoch 4 Batch 2150 Loss 1.1790 Accuracy 0.8158\n",
      "Epoch 4 Batch 2200 Loss 1.1793 Accuracy 0.8158\n",
      "Epoch 4 Batch 2250 Loss 1.1797 Accuracy 0.8157\n",
      "Epoch 4 Batch 2300 Loss 1.1795 Accuracy 0.8157\n",
      "Epoch 4 Batch 2350 Loss 1.1795 Accuracy 0.8157\n",
      "Epoch 4 Batch 2400 Loss 1.1790 Accuracy 0.8157\n",
      "Epoch 4 Batch 2450 Loss 1.1783 Accuracy 0.8158\n",
      "Epoch 4 Batch 2500 Loss 1.1787 Accuracy 0.8158\n",
      "Epoch 4 Batch 2550 Loss 1.1802 Accuracy 0.8155\n",
      "Epoch 4 Batch 2600 Loss 1.1814 Accuracy 0.8154\n",
      "Epoch 4 Batch 2650 Loss 1.1814 Accuracy 0.8155\n",
      "Epoch 4 Batch 2700 Loss 1.1825 Accuracy 0.8152\n",
      "Epoch 4 Batch 2750 Loss 1.1830 Accuracy 0.8151\n",
      "Epoch 4 Batch 2800 Loss 1.1821 Accuracy 0.8153\n",
      "Epoch 4 Batch 2850 Loss 1.1836 Accuracy 0.8151\n",
      "Epoch 4 Batch 2900 Loss 1.1835 Accuracy 0.8151\n",
      "Epoch 4 Batch 2950 Loss 1.1849 Accuracy 0.8149\n",
      "Epoch 4 Batch 3000 Loss 1.1861 Accuracy 0.8146\n",
      "Epoch 4 Batch 3050 Loss 1.1874 Accuracy 0.8145\n",
      "Epoch 4 Batch 3100 Loss 1.1876 Accuracy 0.8145\n",
      "Epoch 4 Batch 3150 Loss 1.1879 Accuracy 0.8145\n",
      "Epoch 4 Batch 3200 Loss 1.1889 Accuracy 0.8143\n",
      "Epoch 4 Batch 3250 Loss 1.1885 Accuracy 0.8144\n",
      "Epoch 4 Batch 3300 Loss 1.1882 Accuracy 0.8144\n",
      "Epoch 4 Batch 3350 Loss 1.1899 Accuracy 0.8142\n",
      "Epoch 4 Batch 3400 Loss 1.1901 Accuracy 0.8142\n",
      "Epoch 4 Batch 3450 Loss 1.1887 Accuracy 0.8144\n",
      "Epoch 4 Batch 3500 Loss 1.1897 Accuracy 0.8142\n",
      "Epoch 4 Batch 3550 Loss 1.1892 Accuracy 0.8143\n",
      "Epoch 4 Batch 3600 Loss 1.1893 Accuracy 0.8143\n",
      "Epoch 4 Batch 3650 Loss 1.1897 Accuracy 0.8142\n",
      "Epoch 4 Batch 3700 Loss 1.1907 Accuracy 0.8141\n",
      "Epoch 4 Batch 3750 Loss 1.1906 Accuracy 0.8141\n",
      "Epoch 4 Batch 3800 Loss 1.1914 Accuracy 0.8141\n",
      "Epoch 4 Batch 3850 Loss 1.1917 Accuracy 0.8141\n",
      "Epoch 4 Batch 3900 Loss 1.1904 Accuracy 0.8143\n",
      "Epoch 4 Batch 3950 Loss 1.1896 Accuracy 0.8144\n",
      "Epoch 4 Batch 4000 Loss 1.1894 Accuracy 0.8144\n",
      "Epoch 4 Batch 4050 Loss 1.1909 Accuracy 0.8143\n",
      "Epoch 4 Batch 4100 Loss 1.1909 Accuracy 0.8143\n",
      "Epoch 4 Batch 4150 Loss 1.1917 Accuracy 0.8142\n",
      "Epoch 4 Batch 4200 Loss 1.1910 Accuracy 0.8143\n",
      "Epoch 4 Batch 4250 Loss 1.1922 Accuracy 0.8142\n",
      "Epoch 4 Batch 4300 Loss 1.1915 Accuracy 0.8143\n",
      "Epoch 4 Batch 4350 Loss 1.1920 Accuracy 0.8142\n",
      "Epoch 4 Batch 4400 Loss 1.1918 Accuracy 0.8142\n",
      "Epoch 4 Batch 4450 Loss 1.1924 Accuracy 0.8141\n",
      "Epoch 4 Batch 4500 Loss 1.1924 Accuracy 0.8142\n",
      "Epoch 4 Batch 4550 Loss 1.1927 Accuracy 0.8141\n",
      "Epoch 4 Batch 4600 Loss 1.1930 Accuracy 0.8140\n",
      "Epoch 4 Batch 4650 Loss 1.1934 Accuracy 0.8140\n",
      "Epoch 4 Batch 4700 Loss 1.1937 Accuracy 0.8140\n",
      "Epoch 4 Batch 4750 Loss 1.1943 Accuracy 0.8139\n",
      "Epoch 4 Batch 4800 Loss 1.1945 Accuracy 0.8139\n",
      "Epoch 4 Batch 4850 Loss 1.1946 Accuracy 0.8139\n",
      "Epoch 4 Batch 4900 Loss 1.1948 Accuracy 0.8138\n",
      "Epoch 4 Batch 4950 Loss 1.1942 Accuracy 0.8139\n",
      "Epoch 4 Batch 5000 Loss 1.1943 Accuracy 0.8139\n",
      "Epoch 4 Batch 5050 Loss 1.1947 Accuracy 0.8138\n",
      "Epoch 4 Batch 5100 Loss 1.1954 Accuracy 0.8138\n",
      "Epoch 4 Batch 5150 Loss 1.1949 Accuracy 0.8138\n",
      "Epoch 4 Batch 5200 Loss 1.1956 Accuracy 0.8138\n",
      "Epoch 4 Batch 5250 Loss 1.1960 Accuracy 0.8137\n",
      "Epoch 4 Batch 5300 Loss 1.1955 Accuracy 0.8138\n",
      "Epoch 4 Batch 5350 Loss 1.1954 Accuracy 0.8138\n",
      "Epoch 4 Batch 5400 Loss 1.1958 Accuracy 0.8138\n",
      "Epoch 4 Batch 5450 Loss 1.1964 Accuracy 0.8137\n",
      "Epoch 4 Batch 5500 Loss 1.1959 Accuracy 0.8137\n",
      "Epoch 4 Batch 5550 Loss 1.1957 Accuracy 0.8138\n",
      "Epoch 4 Batch 5600 Loss 1.1960 Accuracy 0.8137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 06:20:10.958037: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss 1.1958 Accuracy 0.8137\n",
      "Time taken for 1 epoch: 6724.997980117798 secs\n",
      "\n",
      "Epoch 5 Batch 0 Loss 1.9049 Accuracy 0.7556\n",
      "Epoch 5 Batch 50 Loss 0.9830 Accuracy 0.8438\n",
      "Epoch 5 Batch 100 Loss 1.0553 Accuracy 0.8352\n",
      "Epoch 5 Batch 150 Loss 1.0911 Accuracy 0.8310\n",
      "Epoch 5 Batch 200 Loss 1.1172 Accuracy 0.8278\n",
      "Epoch 5 Batch 250 Loss 1.1272 Accuracy 0.8267\n",
      "Epoch 5 Batch 300 Loss 1.1199 Accuracy 0.8273\n",
      "Epoch 5 Batch 350 Loss 1.1244 Accuracy 0.8264\n",
      "Epoch 5 Batch 400 Loss 1.1266 Accuracy 0.8261\n",
      "Epoch 5 Batch 450 Loss 1.1279 Accuracy 0.8258\n",
      "Epoch 5 Batch 500 Loss 1.1187 Accuracy 0.8270\n",
      "Epoch 5 Batch 550 Loss 1.1206 Accuracy 0.8267\n",
      "Epoch 5 Batch 600 Loss 1.1190 Accuracy 0.8269\n",
      "Epoch 5 Batch 650 Loss 1.1151 Accuracy 0.8270\n",
      "Epoch 5 Batch 700 Loss 1.1175 Accuracy 0.8268\n",
      "Epoch 5 Batch 750 Loss 1.1184 Accuracy 0.8266\n",
      "Epoch 5 Batch 800 Loss 1.1160 Accuracy 0.8270\n",
      "Epoch 5 Batch 850 Loss 1.1266 Accuracy 0.8257\n",
      "Epoch 5 Batch 900 Loss 1.1264 Accuracy 0.8257\n",
      "Epoch 5 Batch 950 Loss 1.1249 Accuracy 0.8259\n",
      "Epoch 5 Batch 1000 Loss 1.1229 Accuracy 0.8260\n",
      "Epoch 5 Batch 1050 Loss 1.1265 Accuracy 0.8254\n",
      "Epoch 5 Batch 1100 Loss 1.1266 Accuracy 0.8252\n",
      "Epoch 5 Batch 1150 Loss 1.1274 Accuracy 0.8250\n",
      "Epoch 5 Batch 1200 Loss 1.1251 Accuracy 0.8253\n",
      "Epoch 5 Batch 1250 Loss 1.1234 Accuracy 0.8256\n",
      "Epoch 5 Batch 1300 Loss 1.1240 Accuracy 0.8253\n",
      "Epoch 5 Batch 1350 Loss 1.1252 Accuracy 0.8251\n",
      "Epoch 5 Batch 1400 Loss 1.1234 Accuracy 0.8253\n",
      "Epoch 5 Batch 1450 Loss 1.1266 Accuracy 0.8251\n",
      "Epoch 5 Batch 1500 Loss 1.1285 Accuracy 0.8248\n",
      "Epoch 5 Batch 1550 Loss 1.1283 Accuracy 0.8248\n",
      "Epoch 5 Batch 1600 Loss 1.1272 Accuracy 0.8249\n",
      "Epoch 5 Batch 1650 Loss 1.1260 Accuracy 0.8250\n",
      "Epoch 5 Batch 1700 Loss 1.1252 Accuracy 0.8250\n",
      "Epoch 5 Batch 1750 Loss 1.1254 Accuracy 0.8250\n",
      "Epoch 5 Batch 1800 Loss 1.1254 Accuracy 0.8251\n",
      "Epoch 5 Batch 1850 Loss 1.1263 Accuracy 0.8250\n",
      "Epoch 5 Batch 1900 Loss 1.1292 Accuracy 0.8246\n",
      "Epoch 5 Batch 1950 Loss 1.1293 Accuracy 0.8247\n",
      "Epoch 5 Batch 2000 Loss 1.1312 Accuracy 0.8244\n",
      "Epoch 5 Batch 2050 Loss 1.1339 Accuracy 0.8241\n",
      "Epoch 5 Batch 2100 Loss 1.1347 Accuracy 0.8241\n",
      "Epoch 5 Batch 2150 Loss 1.1363 Accuracy 0.8239\n",
      "Epoch 5 Batch 2200 Loss 1.1382 Accuracy 0.8237\n",
      "Epoch 5 Batch 2250 Loss 1.1372 Accuracy 0.8239\n",
      "Epoch 5 Batch 2300 Loss 1.1375 Accuracy 0.8239\n",
      "Epoch 5 Batch 2350 Loss 1.1393 Accuracy 0.8237\n",
      "Epoch 5 Batch 2400 Loss 1.1400 Accuracy 0.8236\n",
      "Epoch 5 Batch 2450 Loss 1.1432 Accuracy 0.8231\n",
      "Epoch 5 Batch 2500 Loss 1.1453 Accuracy 0.8229\n",
      "Epoch 5 Batch 2550 Loss 1.1461 Accuracy 0.8228\n",
      "Epoch 5 Batch 2600 Loss 1.1457 Accuracy 0.8228\n",
      "Epoch 5 Batch 2650 Loss 1.1469 Accuracy 0.8226\n",
      "Epoch 5 Batch 2700 Loss 1.1481 Accuracy 0.8224\n",
      "Epoch 5 Batch 2750 Loss 1.1474 Accuracy 0.8225\n",
      "Epoch 5 Batch 2800 Loss 1.1470 Accuracy 0.8226\n",
      "Epoch 5 Batch 2850 Loss 1.1463 Accuracy 0.8227\n",
      "Epoch 5 Batch 2900 Loss 1.1480 Accuracy 0.8225\n",
      "Epoch 5 Batch 2950 Loss 1.1483 Accuracy 0.8225\n",
      "Epoch 5 Batch 3000 Loss 1.1475 Accuracy 0.8226\n",
      "Epoch 5 Batch 3050 Loss 1.1474 Accuracy 0.8225\n",
      "Epoch 5 Batch 3100 Loss 1.1470 Accuracy 0.8226\n",
      "Epoch 5 Batch 3150 Loss 1.1478 Accuracy 0.8224\n",
      "Epoch 5 Batch 3200 Loss 1.1483 Accuracy 0.8223\n",
      "Epoch 5 Batch 3250 Loss 1.1498 Accuracy 0.8221\n",
      "Epoch 5 Batch 3300 Loss 1.1501 Accuracy 0.8221\n",
      "Epoch 5 Batch 3350 Loss 1.1494 Accuracy 0.8222\n",
      "Epoch 5 Batch 3400 Loss 1.1496 Accuracy 0.8222\n",
      "Epoch 5 Batch 3450 Loss 1.1498 Accuracy 0.8221\n",
      "Epoch 5 Batch 3500 Loss 1.1488 Accuracy 0.8223\n",
      "Epoch 5 Batch 3550 Loss 1.1490 Accuracy 0.8223\n",
      "Epoch 5 Batch 3600 Loss 1.1501 Accuracy 0.8221\n",
      "Epoch 5 Batch 3650 Loss 1.1499 Accuracy 0.8221\n",
      "Epoch 5 Batch 3700 Loss 1.1500 Accuracy 0.8221\n",
      "Epoch 5 Batch 3750 Loss 1.1504 Accuracy 0.8221\n",
      "Epoch 5 Batch 3800 Loss 1.1517 Accuracy 0.8219\n",
      "Epoch 5 Batch 3850 Loss 1.1516 Accuracy 0.8219\n",
      "Epoch 5 Batch 3900 Loss 1.1512 Accuracy 0.8219\n",
      "Epoch 5 Batch 3950 Loss 1.1515 Accuracy 0.8219\n",
      "Epoch 5 Batch 4000 Loss 1.1516 Accuracy 0.8219\n",
      "Epoch 5 Batch 4050 Loss 1.1512 Accuracy 0.8220\n",
      "Epoch 5 Batch 4100 Loss 1.1524 Accuracy 0.8218\n",
      "Epoch 5 Batch 4150 Loss 1.1512 Accuracy 0.8220\n",
      "Epoch 5 Batch 4200 Loss 1.1511 Accuracy 0.8220\n",
      "Epoch 5 Batch 4250 Loss 1.1503 Accuracy 0.8221\n",
      "Epoch 5 Batch 4300 Loss 1.1519 Accuracy 0.8220\n",
      "Epoch 5 Batch 4350 Loss 1.1514 Accuracy 0.8220\n",
      "Epoch 5 Batch 4400 Loss 1.1527 Accuracy 0.8218\n",
      "Epoch 5 Batch 4450 Loss 1.1536 Accuracy 0.8218\n",
      "Epoch 5 Batch 4500 Loss 1.1526 Accuracy 0.8218\n",
      "Epoch 5 Batch 4550 Loss 1.1526 Accuracy 0.8219\n",
      "Epoch 5 Batch 4600 Loss 1.1522 Accuracy 0.8219\n",
      "Epoch 5 Batch 4650 Loss 1.1525 Accuracy 0.8219\n",
      "Epoch 5 Batch 4700 Loss 1.1538 Accuracy 0.8217\n",
      "Epoch 5 Batch 4750 Loss 1.1524 Accuracy 0.8219\n",
      "Epoch 5 Batch 4800 Loss 1.1526 Accuracy 0.8219\n",
      "Epoch 5 Batch 4850 Loss 1.1533 Accuracy 0.8218\n",
      "Epoch 5 Batch 4900 Loss 1.1526 Accuracy 0.8219\n",
      "Epoch 5 Batch 4950 Loss 1.1534 Accuracy 0.8218\n",
      "Epoch 5 Batch 5000 Loss 1.1532 Accuracy 0.8219\n",
      "Epoch 5 Batch 5050 Loss 1.1525 Accuracy 0.8219\n",
      "Epoch 5 Batch 5100 Loss 1.1528 Accuracy 0.8219\n",
      "Epoch 5 Batch 5150 Loss 1.1535 Accuracy 0.8218\n",
      "Epoch 5 Batch 5200 Loss 1.1534 Accuracy 0.8218\n",
      "Epoch 5 Batch 5250 Loss 1.1548 Accuracy 0.8216\n",
      "Epoch 5 Batch 5300 Loss 1.1545 Accuracy 0.8217\n",
      "Epoch 5 Batch 5350 Loss 1.1539 Accuracy 0.8217\n",
      "Epoch 5 Batch 5400 Loss 1.1538 Accuracy 0.8218\n",
      "Epoch 5 Batch 5450 Loss 1.1538 Accuracy 0.8218\n",
      "Epoch 5 Batch 5500 Loss 1.1540 Accuracy 0.8217\n",
      "Epoch 5 Batch 5550 Loss 1.1550 Accuracy 0.8216\n",
      "Epoch 5 Batch 5600 Loss 1.1550 Accuracy 0.8216\n",
      "Saving checkpoint for epoch 5 at ./ckpt-1\n",
      "Epoch 5 Loss 1.1552 Accuracy 0.8216\n",
      "Time taken for 1 epoch: 6621.154025793076 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# 5 에포크 훈련\n",
    "for epoch in range(5):\n",
    "  start = time.time()\n",
    "  \n",
    "  train_loss.reset_state()\n",
    "  train_accuracy.reset_state()\n",
    "  \n",
    "  # input : 한국어, tar : 영어\n",
    "  for (batch, (inp, tar)) in enumerate(dataset_train):\n",
    "    train_step(inp, tar)\n",
    "    \n",
    "    if batch % 50 == 0:\n",
    "      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
    "          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
    "      \n",
    "  if (epoch + 1) % 5 == 0:\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                         ckpt_save_path))\n",
    "    \n",
    "  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
    "                                                train_loss.result(), \n",
    "                                                train_accuracy.result()))\n",
    "\n",
    "  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "507cf91a-2f48-449a-8b19-3a6417d58370",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator():\n",
    "    def __init__(self, transformer, vectorizer_ko, vectorizer_en):\n",
    "        self.transformer = transformer\n",
    "        self.vectorizer_ko = vectorizer_ko\n",
    "        self.vectorizer_en = vectorizer_en\n",
    "\n",
    "    def __call__(self, sentence, seq_length):\n",
    "        self.start_en = self.vectorizer_en.vocab_size\n",
    "        self.end_en = self.vectorizer_en.vocab_size + 1\n",
    "        self.sentence = tf.expand_dims(self.vectorizer_ko.encode(sentence), axis=0)  # 차원 추가\n",
    "\n",
    "        output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n",
    "        output_array = output_array.write(0, self.start_en)\n",
    "\n",
    "        for i in tf.range(seq_length):\n",
    "            output = tf.transpose(output_array.stack())\n",
    "            predictions = self.transformer(self.sentence, output)\n",
    "\n",
    "            predictions = predictions[:, -1:, :]  \n",
    "            predicted_id = tf.argmax(predictions, axis=-1)\n",
    "\n",
    "            output_array = output_array.write(i + 1, predicted_id[0])\n",
    "\n",
    "            if tf.reduce_all(predicted_id == self.end_en):\n",
    "                break\n",
    "\n",
    "        output = tf.transpose(output_array.stack())\n",
    "        final_output = self.vectorizer_en.decode(tf.squeeze(output))\n",
    "\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc167a15-f455-4d82-865d-ba926b4b1a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator(transformer, vectorizer_ko, vectorizer_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fb970b4f-8cbe-4178-b21f-db5c5ea4159b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"나는 한국인이다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "74cdb55c-b58f-4f45-a88b-260542c9adaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 22:11:11.146419: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at strided_slice_op.cc:117 : INVALID_ARGUMENT: slice index 1 of dimension 0 out of bounds.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:CPU:0}} slice index 1 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice/",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtranslator\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[83], line 17\u001b[0m, in \u001b[0;36mTranslator.__call__\u001b[0;34m(self, sentence, seq_length)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mrange(seq_length):\n\u001b[1;32m     16\u001b[0m     output \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtranspose(output_array\u001b[38;5;241m.\u001b[39mstack())\n\u001b[0;32m---> 17\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m predictions[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:, :]  \n\u001b[1;32m     20\u001b[0m     predicted_id \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39margmax(predictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[100], line 54\u001b[0m, in \u001b[0;36mTransformer.__call__\u001b[0;34m(self, src_lang, tar_lang)\u001b[0m\n\u001b[1;32m     46\u001b[0m out_enc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(src_embedded, pad_mask)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# #tar_lang vectorize\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# tar_slices = tf.data.Dataset.from_tensor_slices(tar_lang)\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# self.vectorizer_tar.adapt(tar_slices)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# tar_lang = self.vectorizer_tar(tar_lang)\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m#vectorized된 input으로 look ahead mask 생성\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m look_ahead_mask \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtar_lang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m#vectorized된 input으로 embedding + positional encoding 실행\u001b[39;00m\n\u001b[1;32m     57\u001b[0m tar_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_tar(tar_lang)\n",
      "Cell \u001b[0;32mIn[96], line 12\u001b[0m, in \u001b[0;36mcreate_mask\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_mask\u001b[39m(x):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m#padding과 look ahead mask를 한번에 만들어주는 함수\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m#input으로 사용되는 x는 vectorized된 문장이다. \u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m#input 형태 : (batch_size, seq_length) -> [[1,2,3,1,0,0,0],[3,2,4,0,0,0,0]] 이런 형태로 입력\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     temp_mask \u001b[38;5;241m=\u001b[39m create_lookaheadmask(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m) \u001b[38;5;66;03m#temp_mask는 위의 create_lookaheadmask를 그대로 실행해서 만듬\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     reverse_tar \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mequal(x, \u001b[38;5;241m0\u001b[39m),dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;66;03m#입력이 되는 tar 문장의 pad mask이다. 즉 pad 부분을 1로, 단어가 있는 부분을 0으로 만드는 식\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     reverse_tar \u001b[38;5;241m=\u001b[39m reverse_tar[:,tf\u001b[38;5;241m.\u001b[39mnewaxis,tf\u001b[38;5;241m.\u001b[39mnewaxis,:] \u001b[38;5;66;03m#나중에 (batch_size, num_head, seq_length, seq_length) 형태로 더해주기 위해 형태 변형 -> (batch_size, 1, 1, seq_length)형태\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/tensorflow/python/framework/ops.py:5983\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   5981\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NoReturn:\n\u001b[1;32m   5982\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m-> 5983\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:CPU:0}} slice index 1 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice/"
     ]
    }
   ],
   "source": [
    "translator(sentence, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f39bf89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
